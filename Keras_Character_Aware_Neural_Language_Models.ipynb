{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/stikbuf/Language_Modeling/blob/master/Keras_Character_Aware_Neural_Language_Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_0CMzmyQXoy"
   },
   "source": [
    "## Configure the cloud environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB7JyNTfQfKF"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2539
    },
    "colab_type": "code",
    "id": "oTB-axrvQiZU",
    "outputId": "e2997ab7-5a01-4406-d696-aae0c2c981aa"
   },
   "outputs": [],
   "source": [
    "# Install a Drive FUSE wrapper.\n",
    "# https://github.com/astrada/google-drive-ocamlfuse\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "\n",
    "# Generate auth tokens for Colab\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "\n",
    "# Generate creds for the Drive FUSE library.\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zVhUtsJjQqy5",
    "outputId": "fbf684be-5cab-471f-ed07-2cc929cc4e7d"
   },
   "outputs": [],
   "source": [
    "# If you got a \"Transport endpoint is not connected.\" error. Please run this line first to unmount the drive.\n",
    "# See https://stackoverflow.com/questions/49588113/google-colab-script-throws-transport-endpoint-is-not-connected?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "!fusermount -u drive\n",
    "\n",
    "# Create a directory and mount Google Drive using that directory.\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "a = !ls drive/\n",
    "print('Files in Drive:', a)\n",
    "assert a!=[], 'Drive should not be empty!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "bvfVOCzkRErx",
    "outputId": "2b10a93a-4be4-457f-ff93-749ace078ab4"
   },
   "outputs": [],
   "source": [
    "local_path='./drive/share_with_me/AI/Character-aware_LM/'\n",
    "#local_path='./'\n",
    "import sys\n",
    "sys.path.append(local_path)\n",
    "!ls './drive/share_with_me/AI/Character-aware_LM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TbWRANsEQr6U",
    "outputId": "fbedffd8-9029-4ca3-c73c-b87c6a74bfe6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#assert tf.test.gpu_device_name() != '', \"GPU not avaliable!\"\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUldvgY1RH0S"
   },
   "source": [
    "## Load data (Penn Tree bank -- PTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NgfjuJbPoz9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reader import ptb_raw_data, ptb_producer # by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dpHEJHNnPo0U",
    "outputId": "78431c7b-a03d-4609-a381-9d7aa5b96632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data, test_data, word_to_id = ptb_raw_data(local_path + 'data') # tokens\n",
    "id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
    "voc_size = len(id_to_word)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 929589, Valid data size: 73760, Test data size: 82430\n",
      "\n",
      "train/val/test_data is a list, some elements in train_data is [9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983]\n"
     ]
    }
   ],
   "source": [
    "print('Train data size: {0}, Valid data size: {1}, Test data size: {2}\\n'.\n",
    "      format(len(train_data), len(valid_data), len(test_data)))\n",
    "print('train/val/test_data is a list, some elements in train_data is', train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oZ8sUleQSoWQ",
    "outputId": "c4796a76-4fe3-48a3-f62a-bef541f937b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[voc_size]='<SS>' # Add start word token '<SS>'\n",
    "id_to_word[voc_size+1]='<EE>' # Add end word token '<EE>'\n",
    "word_to_id = dict((v, k) for k, v in id_to_word.items())\n",
    "voc_size = len(id_to_word)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "zj8IWqJ8Po0h",
    "outputId": "5e9fded6-d6d9-44f1-dfa6-9fdece6f2df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id\n",
      "the     0\n",
      "<unk>   1\n",
      "<eos>   2\n",
      "N       3\n",
      "of      4\n",
      "              id\n",
      "ssangyong   9997\n",
      "swapo       9998\n",
      "wachter     9999\n",
      "<SS>       10000\n",
      "<EE>       10001\n"
     ]
    }
   ],
   "source": [
    "word_id = pd.DataFrame.from_dict(word_to_id, orient='index').sort_values(by=0, ascending=True)\n",
    "word_id.columns = ['id']\n",
    "print(word_id.head())\n",
    "print(word_id.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "uxPQPQqMPo0t",
    "outputId": "946af7f2-7906-4fa3-f2bc-d1d7bffe5428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word\n",
      "0    the\n",
      "1  <unk>\n",
      "2  <eos>\n",
      "3      N\n",
      "4     of\n",
      "            word\n",
      "9997   ssangyong\n",
      "9998       swapo\n",
      "9999     wachter\n",
      "10000       <SS>\n",
      "10001       <EE>\n"
     ]
    }
   ],
   "source": [
    "id_word = pd.DataFrame.from_dict(id_to_word, orient='index')\n",
    "id_word.columns = ['word']\n",
    "print(id_word.head())\n",
    "print(id_word.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Hz5IRBM6Po04",
    "outputId": "3d885c2d-15f3-4176-e943-1ae2ed489515"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk> N years old will join the board as a nonexecutive director nov. N <eos> mr. <unk> is chairman of <unk> n.v. the dutch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([id_to_word[id] for id in train_data[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDwfc2jpPo1D"
   },
   "source": [
    "# RNN baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPQMjhyePo1F"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical \n",
    "\n",
    "def gen_word_word(batch_size=128, dataset='train'):\n",
    "    assert dataset in ['train', 'valid', 'test'], 'Dataset must be train or valid or test.'\n",
    "    \n",
    "    dic = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    data = dic[dataset]\n",
    "    \n",
    "    while True:\n",
    "        rnd_idxs = list(range(len(data)-seq_len-1))\n",
    "        random.shuffle(rnd_idxs)\n",
    "        cnt = 0\n",
    "        while cnt < len(rnd_idxs) - batch_size :\n",
    "            X = np.array([[word_to_id['<SS>']] + data[i:i+seq_len] + [word_to_id['<EE>']]\n",
    "                          for i in rnd_idxs[cnt:cnt+batch_size]])\n",
    "            Y = X[:,1:]\n",
    "            X = X[:,:-1]\n",
    "            Y = to_categorical(Y)\n",
    "            #print(X.shape)\n",
    "            cnt += batch_size\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JL6Za0iNPo1O"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import GRU, Dense, Embedding, InputLayer, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout between layers, see [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt6IvftHPo1Y"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "\n",
    "model.add(Embedding(input_dim=voc_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='inputEmbedding'))\n",
    "model.add(GRU(units=128, return_sequences=True))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(GRU(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(voc_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Evcq5fYye8jt"
   },
   "outputs": [],
   "source": [
    "# perplexity\n",
    "def PPL(y_true, y_pred):\n",
    "    return tf.exp(tf.reduce_mean(tf.keras.backend.categorical_crossentropy(y_true, y_pred)))\n",
    "\n",
    "def ACC(y_true, y_pred):\n",
    "    ACC = tf.equal(tf.argmax(y_true, axis = 2), \n",
    "                   tf.argmax(y_pred, axis = 2))\n",
    "    ACC = tf.cast(ACC, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRoe-64bZAWz"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[ACC, PPL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "efPbWtAJPo2P",
    "outputId": "c9bf68c6-3436-44f3-e4e4-04b33a82dfd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputEmbedding (Embedding)   (None, None, 128)         1280256   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 64)          37056     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 10002)       650130    \n",
      "=================================================================\n",
      "Total params: 2,066,130\n",
      "Trainable params: 2,066,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import os\n",
    "if not os.path.exists(local_path + 'model/'):\n",
    "    os.mkdir(local_path + 'model/')\n",
    "\n",
    "path_model = local_path + 'model/model.keras'    \n",
    "tensorboard = TensorBoard(log_dir='log')\n",
    "checkpoint = ModelCheckpoint(filepath=path_model, verbose=1,\n",
    "                             monitor='val_PPL',mode='min' ,save_best_only='True')\n",
    "# path_model = local_path + 'model/model.keras'\n",
    "# model.save(path_model)\n",
    "\n",
    "callback_lists=[tensorboard,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3677
    },
    "colab_type": "code",
    "id": "7rWihlHePo2Z",
    "outputId": "19ef2c40-5173-42d2-e4ff-d43ff273cd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 7.4025 - ACC: 0.0299 - PPL: 2238.6179\n",
      "Epoch 00001: val_PPL improved from inf to 729.31307, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 55s 1s/step - loss: 7.3870 - ACC: 0.0305 - PPL: 2208.9620 - val_loss: 6.5916 - val_ACC: 0.0547 - val_PPL: 729.3131\n",
      "Epoch 2/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.5222 - ACC: 0.0527 - PPL: 680.8545\n",
      "Epoch 00002: val_PPL improved from 729.31307 to 656.51662, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.5233 - ACC: 0.0528 - PPL: 681.6252 - val_loss: 6.4862 - val_ACC: 0.0551 - val_PPL: 656.5166\n",
      "Epoch 3/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.5067 - ACC: 0.0540 - PPL: 670.2737\n",
      "Epoch 00003: val_PPL improved from 656.51662 to 649.24152, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 52s 1s/step - loss: 6.5061 - ACC: 0.0542 - PPL: 669.8924 - val_loss: 6.4749 - val_ACC: 0.0610 - val_PPL: 649.2415\n",
      "Epoch 4/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.4499 - ACC: 0.0648 - PPL: 633.3455\n",
      "Epoch 00004: val_PPL improved from 649.24152 to 604.99419, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 971ms/step - loss: 6.4498 - ACC: 0.0649 - PPL: 633.2387 - val_loss: 6.4041 - val_ACC: 0.0653 - val_PPL: 604.9942\n",
      "Epoch 5/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3868 - ACC: 0.0735 - PPL: 594.6345\n",
      "Epoch 00005: val_PPL improved from 604.99419 to 562.52958, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.3857 - ACC: 0.0736 - PPL: 593.9719 - val_loss: 6.3311 - val_ACC: 0.0865 - val_PPL: 562.5296\n",
      "Epoch 6/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3105 - ACC: 0.0879 - PPL: 550.8956\n",
      "Epoch 00006: val_PPL improved from 562.52958 to 543.89324, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.3095 - ACC: 0.0880 - PPL: 550.3092 - val_loss: 6.2976 - val_ACC: 0.0960 - val_PPL: 543.8932\n",
      "Epoch 7/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.2349 - ACC: 0.1018 - PPL: 510.7661\n",
      "Epoch 00007: val_PPL improved from 543.89324 to 487.47160, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 974ms/step - loss: 6.2331 - ACC: 0.1020 - PPL: 509.8584 - val_loss: 6.1879 - val_ACC: 0.1118 - val_PPL: 487.4716\n",
      "Epoch 8/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1627 - ACC: 0.1119 - PPL: 475.4521\n",
      "Epoch 00008: val_PPL improved from 487.47160 to 456.62706, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.1609 - ACC: 0.1121 - PPL: 474.6371 - val_loss: 6.1225 - val_ACC: 0.1177 - val_PPL: 456.6271\n",
      "Epoch 9/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0888 - ACC: 0.1191 - PPL: 441.5484\n",
      "Epoch 00009: val_PPL improved from 456.62706 to 423.12172, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.0874 - ACC: 0.1192 - PPL: 440.9254 - val_loss: 6.0463 - val_ACC: 0.1232 - val_PPL: 423.1217\n",
      "Epoch 10/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0156 - ACC: 0.1255 - PPL: 410.4352\n",
      "Epoch 00010: val_PPL improved from 423.12172 to 397.22348, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.0170 - ACC: 0.1255 - PPL: 410.9762 - val_loss: 5.9831 - val_ACC: 0.1267 - val_PPL: 397.2235\n",
      "Epoch 11/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9734 - ACC: 0.1313 - PPL: 393.3070\n",
      "Epoch 00011: val_PPL improved from 397.22348 to 386.95126, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.9742 - ACC: 0.1312 - PPL: 393.6298 - val_loss: 5.9565 - val_ACC: 0.1302 - val_PPL: 386.9513\n",
      "Epoch 12/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9391 - ACC: 0.1355 - PPL: 380.3861\n",
      "Epoch 00012: val_PPL improved from 386.95126 to 375.40058, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.9404 - ACC: 0.1353 - PPL: 380.8729 - val_loss: 5.9265 - val_ACC: 0.1407 - val_PPL: 375.4006\n",
      "Epoch 13/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8889 - ACC: 0.1390 - PPL: 361.5832\n",
      "Epoch 00013: val_PPL improved from 375.40058 to 348.37696, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.8874 - ACC: 0.1389 - PPL: 361.0691 - val_loss: 5.8519 - val_ACC: 0.1486 - val_PPL: 348.3770\n",
      "Epoch 14/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8501 - ACC: 0.1447 - PPL: 347.9056\n",
      "Epoch 00014: val_PPL improved from 348.37696 to 333.67055, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.8486 - ACC: 0.1448 - PPL: 347.3916 - val_loss: 5.8081 - val_ACC: 0.1517 - val_PPL: 333.6706\n",
      "Epoch 15/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8114 - ACC: 0.1471 - PPL: 334.4995\n",
      "Epoch 00015: val_PPL improved from 333.67055 to 325.36930, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 985ms/step - loss: 5.8112 - ACC: 0.1472 - PPL: 334.4446 - val_loss: 5.7832 - val_ACC: 0.1513 - val_PPL: 325.3693\n",
      "Epoch 16/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7789 - ACC: 0.1500 - PPL: 323.8882\n",
      "Epoch 00016: val_PPL improved from 325.36930 to 314.00920, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.7804 - ACC: 0.1500 - PPL: 324.3939 - val_loss: 5.7475 - val_ACC: 0.1568 - val_PPL: 314.0092\n",
      "Epoch 17/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7537 - ACC: 0.1519 - PPL: 315.8327\n",
      "Epoch 00017: val_PPL improved from 314.00920 to 308.17599, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.7532 - ACC: 0.1521 - PPL: 315.6607 - val_loss: 5.7289 - val_ACC: 0.1604 - val_PPL: 308.1760\n",
      "Epoch 18/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7405 - ACC: 0.1536 - PPL: 311.8603\n",
      "Epoch 00018: val_PPL improved from 308.17599 to 301.13895, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.7414 - ACC: 0.1535 - PPL: 312.1449 - val_loss: 5.7051 - val_ACC: 0.1630 - val_PPL: 301.1389\n",
      "Epoch 19/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6998 - ACC: 0.1574 - PPL: 299.1991\n",
      "Epoch 00019: val_PPL improved from 301.13895 to 298.01215, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.7018 - ACC: 0.1573 - PPL: 299.8010 - val_loss: 5.6959 - val_ACC: 0.1607 - val_PPL: 298.0122\n",
      "Epoch 20/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6859 - ACC: 0.1588 - PPL: 295.3874\n",
      "Epoch 00020: val_PPL improved from 298.01215 to 285.81103, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.6845 - ACC: 0.1588 - PPL: 294.9730 - val_loss: 5.6535 - val_ACC: 0.1650 - val_PPL: 285.8110\n",
      "Epoch 21/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6441 - ACC: 0.1607 - PPL: 283.1733\n",
      "Epoch 00021: val_PPL improved from 285.81103 to 285.39072, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 994ms/step - loss: 5.6414 - ACC: 0.1609 - PPL: 282.4456 - val_loss: 5.6519 - val_ACC: 0.1674 - val_PPL: 285.3907\n",
      "Epoch 22/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6282 - ACC: 0.1644 - PPL: 278.7390\n",
      "Epoch 00022: val_PPL improved from 285.39072 to 272.89878, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.6290 - ACC: 0.1644 - PPL: 278.9714 - val_loss: 5.6072 - val_ACC: 0.1751 - val_PPL: 272.8988\n",
      "Epoch 23/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6140 - ACC: 0.1648 - PPL: 274.6704\n",
      "Epoch 00023: val_PPL did not improve\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.6144 - ACC: 0.1648 - PPL: 274.7862 - val_loss: 5.6206 - val_ACC: 0.1667 - val_PPL: 276.8625\n",
      "Epoch 24/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5912 - ACC: 0.1686 - PPL: 268.5093\n",
      "Epoch 00024: val_PPL improved from 272.89878 to 264.91221, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.5914 - ACC: 0.1685 - PPL: 268.5535 - val_loss: 5.5774 - val_ACC: 0.1753 - val_PPL: 264.9122\n",
      "Epoch 25/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5866 - ACC: 0.1677 - PPL: 267.2568\n",
      "Epoch 00025: val_PPL improved from 264.91221 to 262.39913, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 958ms/step - loss: 5.5886 - ACC: 0.1673 - PPL: 267.8107 - val_loss: 5.5672 - val_ACC: 0.1775 - val_PPL: 262.3991\n",
      "Epoch 26/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5698 - ACC: 0.1702 - PPL: 262.7644\n",
      "Epoch 00026: val_PPL improved from 262.39913 to 257.71197, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.5693 - ACC: 0.1701 - PPL: 262.6315 - val_loss: 5.5499 - val_ACC: 0.1781 - val_PPL: 257.7120\n",
      "Epoch 27/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5527 - ACC: 0.1712 - PPL: 258.5783\n",
      "Epoch 00027: val_PPL improved from 257.71197 to 251.61956, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.5517 - ACC: 0.1714 - PPL: 258.3164 - val_loss: 5.5260 - val_ACC: 0.1817 - val_PPL: 251.6196\n",
      "Epoch 28/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5348 - ACC: 0.1733 - PPL: 254.0451\n",
      "Epoch 00028: val_PPL improved from 251.61956 to 248.79050, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 52s 1s/step - loss: 5.5351 - ACC: 0.1731 - PPL: 254.1087 - val_loss: 5.5148 - val_ACC: 0.1807 - val_PPL: 248.7905\n",
      "Epoch 29/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5199 - ACC: 0.1734 - PPL: 249.9035\n",
      "Epoch 00029: val_PPL improved from 248.79050 to 244.28281, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.5190 - ACC: 0.1734 - PPL: 249.6854 - val_loss: 5.4957 - val_ACC: 0.1851 - val_PPL: 244.2828\n",
      "Epoch 30/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4971 - ACC: 0.1771 - PPL: 244.3847\n",
      "Epoch 00030: val_PPL improved from 244.28281 to 240.53591, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4966 - ACC: 0.1770 - PPL: 244.2578 - val_loss: 5.4809 - val_ACC: 0.1835 - val_PPL: 240.5359\n",
      "Epoch 31/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5011 - ACC: 0.1768 - PPL: 245.4008\n",
      "Epoch 00031: val_PPL did not improve\n",
      "50/50 [==============================] - 50s 992ms/step - loss: 5.4983 - ACC: 0.1770 - PPL: 244.7443 - val_loss: 5.4807 - val_ACC: 0.1853 - val_PPL: 240.6217\n",
      "Epoch 32/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4852 - ACC: 0.1770 - PPL: 241.5776\n",
      "Epoch 00032: val_PPL improved from 240.53591 to 239.42074, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 973ms/step - loss: 5.4861 - ACC: 0.1771 - PPL: 241.7754 - val_loss: 5.4760 - val_ACC: 0.1814 - val_PPL: 239.4207\n",
      "Epoch 33/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4726 - ACC: 0.1805 - PPL: 238.7435\n",
      "Epoch 00033: val_PPL improved from 239.42074 to 234.69388, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 965ms/step - loss: 5.4706 - ACC: 0.1806 - PPL: 238.2781 - val_loss: 5.4563 - val_ACC: 0.1878 - val_PPL: 234.6939\n",
      "Epoch 34/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4679 - ACC: 0.1803 - PPL: 237.4297\n",
      "Epoch 00034: val_PPL improved from 234.69388 to 233.52022, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4677 - ACC: 0.1803 - PPL: 237.3770 - val_loss: 5.4509 - val_ACC: 0.1901 - val_PPL: 233.5202\n",
      "Epoch 35/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4459 - ACC: 0.1814 - PPL: 232.3040\n",
      "Epoch 00035: val_PPL improved from 233.52022 to 228.76066, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 52s 1s/step - loss: 5.4471 - ACC: 0.1814 - PPL: 232.5757 - val_loss: 5.4304 - val_ACC: 0.1914 - val_PPL: 228.7607\n",
      "Epoch 36/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4395 - ACC: 0.1817 - PPL: 230.7606\n",
      "Epoch 00036: val_PPL improved from 228.76066 to 225.69043, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4415 - ACC: 0.1815 - PPL: 231.2165 - val_loss: 5.4155 - val_ACC: 0.1929 - val_PPL: 225.6904\n",
      "Epoch 37/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4158 - ACC: 0.1850 - PPL: 225.4565\n",
      "Epoch 00037: val_PPL did not improve\n",
      "50/50 [==============================] - 49s 990ms/step - loss: 5.4141 - ACC: 0.1851 - PPL: 225.0778 - val_loss: 5.4335 - val_ACC: 0.1887 - val_PPL: 229.3768\n",
      "Epoch 38/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4302 - ACC: 0.1842 - PPL: 228.5462\n",
      "Epoch 00038: val_PPL improved from 225.69043 to 224.76569, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4296 - ACC: 0.1840 - PPL: 228.4052 - val_loss: 5.4122 - val_ACC: 0.1920 - val_PPL: 224.7657\n",
      "Epoch 39/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4064 - ACC: 0.1851 - PPL: 223.2234\n",
      "Epoch 00039: val_PPL did not improve\n",
      "50/50 [==============================] - 49s 984ms/step - loss: 5.4048 - ACC: 0.1850 - PPL: 222.8637 - val_loss: 5.4252 - val_ACC: 0.1890 - val_PPL: 227.5504\n",
      "Epoch 40/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3942 - ACC: 0.1871 - PPL: 220.5582\n",
      "Epoch 00040: val_PPL improved from 224.76569 to 217.70461, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3949 - ACC: 0.1871 - PPL: 220.7048 - val_loss: 5.3811 - val_ACC: 0.1910 - val_PPL: 217.7046\n",
      "Epoch 41/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3816 - ACC: 0.1883 - PPL: 217.7992\n",
      "Epoch 00041: val_PPL improved from 217.70461 to 216.63400, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.3808 - ACC: 0.1884 - PPL: 217.6164 - val_loss: 5.3757 - val_ACC: 0.1944 - val_PPL: 216.6340\n",
      "Epoch 42/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3697 - ACC: 0.1892 - PPL: 215.1600\n",
      "Epoch 00042: val_PPL improved from 216.63400 to 216.04502, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 968ms/step - loss: 5.3716 - ACC: 0.1889 - PPL: 215.5753 - val_loss: 5.3736 - val_ACC: 0.2002 - val_PPL: 216.0450\n",
      "Epoch 43/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3537 - ACC: 0.1908 - PPL: 211.9230\n",
      "Epoch 00043: val_PPL improved from 216.04502 to 214.37846, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3554 - ACC: 0.1905 - PPL: 212.2878 - val_loss: 5.3659 - val_ACC: 0.1958 - val_PPL: 214.3785\n",
      "Epoch 44/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3676 - ACC: 0.1899 - PPL: 214.6995\n",
      "Epoch 00044: val_PPL improved from 214.37846 to 211.77059, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3680 - ACC: 0.1899 - PPL: 214.7734 - val_loss: 5.3527 - val_ACC: 0.1950 - val_PPL: 211.7706\n",
      "Epoch 45/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3477 - ACC: 0.1909 - PPL: 210.4896\n",
      "Epoch 00045: val_PPL improved from 211.77059 to 206.68730, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3473 - ACC: 0.1909 - PPL: 210.4060 - val_loss: 5.3287 - val_ACC: 0.2024 - val_PPL: 206.6873\n",
      "Epoch 46/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3382 - ACC: 0.1921 - PPL: 208.5513\n",
      "Epoch 00046: val_PPL did not improve\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3389 - ACC: 0.1920 - PPL: 208.6750 - val_loss: 5.3356 - val_ACC: 0.2018 - val_PPL: 208.1482\n",
      "Epoch 47/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3463 - ACC: 0.1933 - PPL: 210.1937\n",
      "Epoch 00047: val_PPL did not improve\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.3460 - ACC: 0.1933 - PPL: 210.1287 - val_loss: 5.3417 - val_ACC: 0.2004 - val_PPL: 209.4001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3376 - ACC: 0.1931 - PPL: 208.4806\n",
      "Epoch 00048: val_PPL improved from 206.68730 to 203.56581, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 957ms/step - loss: 5.3391 - ACC: 0.1930 - PPL: 208.8041 - val_loss: 5.3132 - val_ACC: 0.2039 - val_PPL: 203.5658\n",
      "Epoch 49/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3089 - ACC: 0.1957 - PPL: 202.6321\n",
      "Epoch 00049: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 879ms/step - loss: 5.3088 - ACC: 0.1956 - PPL: 202.5901 - val_loss: 5.3311 - val_ACC: 0.1997 - val_PPL: 207.3809\n",
      "Epoch 50/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3226 - ACC: 0.1943 - PPL: 205.4597\n",
      "Epoch 00050: val_PPL improved from 203.56581 to 197.65276, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 43s 852ms/step - loss: 5.3217 - ACC: 0.1945 - PPL: 205.2664 - val_loss: 5.2845 - val_ACC: 0.2090 - val_PPL: 197.6528\n",
      "Epoch 51/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2976 - ACC: 0.1970 - PPL: 200.4175\n",
      "Epoch 00051: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 868ms/step - loss: 5.2967 - ACC: 0.1970 - PPL: 200.2395 - val_loss: 5.2857 - val_ACC: 0.2072 - val_PPL: 197.8833\n",
      "Epoch 52/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2983 - ACC: 0.1985 - PPL: 200.4826\n",
      "Epoch 00052: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 886ms/step - loss: 5.2987 - ACC: 0.1985 - PPL: 200.5415 - val_loss: 5.2907 - val_ACC: 0.2077 - val_PPL: 198.9466\n",
      "Epoch 53/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2936 - ACC: 0.1980 - PPL: 199.4159\n",
      "Epoch 00053: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 858ms/step - loss: 5.2931 - ACC: 0.1980 - PPL: 199.2994 - val_loss: 5.2986 - val_ACC: 0.2053 - val_PPL: 200.4542\n",
      "Epoch 54/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2910 - ACC: 0.1970 - PPL: 198.8535\n",
      "Epoch 00054: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 858ms/step - loss: 5.2919 - ACC: 0.1969 - PPL: 199.0458 - val_loss: 5.3025 - val_ACC: 0.2075 - val_PPL: 201.2577\n",
      "Epoch 55/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2814 - ACC: 0.2001 - PPL: 197.0094\n",
      "Epoch 00055: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 861ms/step - loss: 5.2805 - ACC: 0.2002 - PPL: 196.8265 - val_loss: 5.2875 - val_ACC: 0.2037 - val_PPL: 198.3650\n",
      "Epoch 56/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2769 - ACC: 0.1986 - PPL: 196.2050\n",
      "Epoch 00056: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 870ms/step - loss: 5.2766 - ACC: 0.1986 - PPL: 196.1556 - val_loss: 5.2944 - val_ACC: 0.2054 - val_PPL: 199.5631\n",
      "Epoch 57/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2737 - ACC: 0.1995 - PPL: 195.5004\n",
      "Epoch 00057: val_PPL improved from 197.65276 to 195.99028, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 872ms/step - loss: 5.2737 - ACC: 0.1995 - PPL: 195.4971 - val_loss: 5.2755 - val_ACC: 0.2109 - val_PPL: 195.9903\n",
      "Epoch 58/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2429 - ACC: 0.2027 - PPL: 189.8190\n",
      "Epoch 00058: val_PPL did not improve\n",
      "50/50 [==============================] - 42s 837ms/step - loss: 5.2420 - ACC: 0.2028 - PPL: 189.6365 - val_loss: 5.2829 - val_ACC: 0.2050 - val_PPL: 197.4012\n",
      "Epoch 59/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2422 - ACC: 0.2026 - PPL: 189.4378\n",
      "Epoch 00059: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 852ms/step - loss: 5.2425 - ACC: 0.2024 - PPL: 189.4854 - val_loss: 5.2771 - val_ACC: 0.2021 - val_PPL: 196.1804\n",
      "Epoch 60/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2599 - ACC: 0.2026 - PPL: 192.8499\n",
      "Epoch 00060: val_PPL improved from 195.99028 to 189.20414, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 43s 853ms/step - loss: 5.2599 - ACC: 0.2027 - PPL: 192.8379 - val_loss: 5.2401 - val_ACC: 0.2139 - val_PPL: 189.2041\n",
      "Epoch 61/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2587 - ACC: 0.2025 - PPL: 192.7077\n",
      "Epoch 00061: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 885ms/step - loss: 5.2586 - ACC: 0.2025 - PPL: 192.6800 - val_loss: 5.2904 - val_ACC: 0.2010 - val_PPL: 198.8837\n",
      "Epoch 62/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2225 - ACC: 0.2059 - PPL: 185.7580\n",
      "Epoch 00062: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 882ms/step - loss: 5.2237 - ACC: 0.2058 - PPL: 185.9833 - val_loss: 5.2549 - val_ACC: 0.2116 - val_PPL: 191.8594\n",
      "Epoch 63/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2287 - ACC: 0.2046 - PPL: 186.7993\n",
      "Epoch 00063: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 865ms/step - loss: 5.2280 - ACC: 0.2046 - PPL: 186.6704 - val_loss: 5.2444 - val_ACC: 0.2129 - val_PPL: 189.9324\n",
      "Epoch 64/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2229 - ACC: 0.2046 - PPL: 185.9584\n",
      "Epoch 00064: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 863ms/step - loss: 5.2222 - ACC: 0.2045 - PPL: 185.8074 - val_loss: 5.2594 - val_ACC: 0.2096 - val_PPL: 192.6372\n",
      "Epoch 65/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2206 - ACC: 0.2059 - PPL: 185.5133\n",
      "Epoch 00065: val_PPL improved from 189.20414 to 181.13306, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 881ms/step - loss: 5.2197 - ACC: 0.2060 - PPL: 185.3405 - val_loss: 5.1968 - val_ACC: 0.2191 - val_PPL: 181.1331\n",
      "Epoch 66/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2220 - ACC: 0.2055 - PPL: 185.6233\n",
      "Epoch 00066: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 866ms/step - loss: 5.2224 - ACC: 0.2055 - PPL: 185.6783 - val_loss: 5.2178 - val_ACC: 0.2170 - val_PPL: 184.8569\n",
      "Epoch 67/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2159 - ACC: 0.2075 - PPL: 184.6417\n",
      "Epoch 00067: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 878ms/step - loss: 5.2162 - ACC: 0.2075 - PPL: 184.7031 - val_loss: 5.2194 - val_ACC: 0.2094 - val_PPL: 185.1529\n",
      "Epoch 68/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2086 - ACC: 0.2064 - PPL: 183.2449\n",
      "Epoch 00068: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 887ms/step - loss: 5.2084 - ACC: 0.2064 - PPL: 183.2051 - val_loss: 5.2230 - val_ACC: 0.2130 - val_PPL: 185.9379\n",
      "Epoch 69/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2056 - ACC: 0.2055 - PPL: 182.5848\n",
      "Epoch 00069: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 857ms/step - loss: 5.2065 - ACC: 0.2057 - PPL: 182.7500 - val_loss: 5.2227 - val_ACC: 0.2133 - val_PPL: 185.7875\n",
      "Epoch 70/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2143 - ACC: 0.2052 - PPL: 184.3099\n",
      "Epoch 00070: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 901ms/step - loss: 5.2159 - ACC: 0.2052 - PPL: 184.6133 - val_loss: 5.2040 - val_ACC: 0.2133 - val_PPL: 182.3497\n",
      "Epoch 71/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1867 - ACC: 0.2094 - PPL: 179.1089\n",
      "Epoch 00071: val_PPL improved from 181.13306 to 178.13573, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 888ms/step - loss: 5.1881 - ACC: 0.2093 - PPL: 179.3580 - val_loss: 5.1811 - val_ACC: 0.2202 - val_PPL: 178.1357\n",
      "Epoch 72/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1775 - ACC: 0.2091 - PPL: 177.6599\n",
      "Epoch 00072: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 867ms/step - loss: 5.1765 - ACC: 0.2092 - PPL: 177.4871 - val_loss: 5.2066 - val_ACC: 0.2173 - val_PPL: 182.8368\n",
      "Epoch 73/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1861 - ACC: 0.2096 - PPL: 179.1604\n",
      "Epoch 00073: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 863ms/step - loss: 5.1878 - ACC: 0.2095 - PPL: 179.4650 - val_loss: 5.2073 - val_ACC: 0.2149 - val_PPL: 183.1281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1701 - ACC: 0.2105 - PPL: 176.2739\n",
      "Epoch 00074: val_PPL improved from 178.13573 to 176.60325, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 42s 849ms/step - loss: 5.1714 - ACC: 0.2102 - PPL: 176.5084 - val_loss: 5.1722 - val_ACC: 0.2181 - val_PPL: 176.6033\n",
      "Epoch 75/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1834 - ACC: 0.2090 - PPL: 178.6776\n",
      "Epoch 00075: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 896ms/step - loss: 5.1865 - ACC: 0.2085 - PPL: 179.2699 - val_loss: 5.2059 - val_ACC: 0.2142 - val_PPL: 182.8677\n",
      "Epoch 76/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1875 - ACC: 0.2086 - PPL: 179.4254\n",
      "Epoch 00076: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 856ms/step - loss: 5.1876 - ACC: 0.2086 - PPL: 179.4425 - val_loss: 5.1926 - val_ACC: 0.2175 - val_PPL: 180.5738\n",
      "Epoch 77/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1857 - ACC: 0.2081 - PPL: 178.9215\n",
      "Epoch 00077: val_PPL improved from 176.60325 to 174.83749, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 45s 895ms/step - loss: 5.1845 - ACC: 0.2082 - PPL: 178.6979 - val_loss: 5.1617 - val_ACC: 0.2213 - val_PPL: 174.8375\n",
      "Epoch 78/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1729 - ACC: 0.2096 - PPL: 176.8041\n",
      "Epoch 00078: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 900ms/step - loss: 5.1724 - ACC: 0.2096 - PPL: 176.7159 - val_loss: 5.1841 - val_ACC: 0.2183 - val_PPL: 179.0569\n",
      "Epoch 79/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1538 - ACC: 0.2111 - PPL: 173.6284\n",
      "Epoch 00079: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 867ms/step - loss: 5.1541 - ACC: 0.2111 - PPL: 173.6729 - val_loss: 5.1707 - val_ACC: 0.2190 - val_PPL: 176.4778\n",
      "Epoch 80/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1438 - ACC: 0.2123 - PPL: 171.7268\n",
      "Epoch 00080: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 880ms/step - loss: 5.1430 - ACC: 0.2125 - PPL: 171.5767 - val_loss: 5.1704 - val_ACC: 0.2175 - val_PPL: 176.2964\n",
      "Epoch 81/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1660 - ACC: 0.2111 - PPL: 175.6476\n",
      "Epoch 00081: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 870ms/step - loss: 5.1653 - ACC: 0.2111 - PPL: 175.5167 - val_loss: 5.1930 - val_ACC: 0.2167 - val_PPL: 180.4240\n",
      "Epoch 82/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1615 - ACC: 0.2117 - PPL: 174.7063\n",
      "Epoch 00082: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 857ms/step - loss: 5.1617 - ACC: 0.2116 - PPL: 174.7436 - val_loss: 5.1752 - val_ACC: 0.2181 - val_PPL: 177.1993\n",
      "Epoch 83/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1480 - ACC: 0.2117 - PPL: 172.3953\n",
      "Epoch 00083: val_PPL improved from 174.83749 to 174.69161, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 871ms/step - loss: 5.1452 - ACC: 0.2118 - PPL: 171.9518 - val_loss: 5.1594 - val_ACC: 0.2215 - val_PPL: 174.6916\n",
      "Epoch 84/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1602 - ACC: 0.2121 - PPL: 174.6343\n",
      "Epoch 00084: val_PPL improved from 174.69161 to 172.52272, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 877ms/step - loss: 5.1600 - ACC: 0.2121 - PPL: 174.5956 - val_loss: 5.1475 - val_ACC: 0.2222 - val_PPL: 172.5227\n",
      "Epoch 85/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1394 - ACC: 0.2143 - PPL: 170.9948\n",
      "Epoch 00085: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 888ms/step - loss: 5.1398 - ACC: 0.2141 - PPL: 171.0487 - val_loss: 5.1654 - val_ACC: 0.2203 - val_PPL: 175.4580\n",
      "Epoch 86/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1406 - ACC: 0.2139 - PPL: 171.3114\n",
      "Epoch 00086: val_PPL did not improve\n",
      "50/50 [==============================] - 46s 925ms/step - loss: 5.1408 - ACC: 0.2139 - PPL: 171.3271 - val_loss: 5.1684 - val_ACC: 0.2197 - val_PPL: 176.0250\n",
      "Epoch 87/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1338 - ACC: 0.2143 - PPL: 170.0832\n",
      "Epoch 00087: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 889ms/step - loss: 5.1350 - ACC: 0.2141 - PPL: 170.2737 - val_loss: 5.1763 - val_ACC: 0.2151 - val_PPL: 177.2997\n",
      "Epoch 88/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1430 - ACC: 0.2135 - PPL: 171.7424\n",
      "Epoch 00088: val_PPL improved from 172.52272 to 171.42467, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 879ms/step - loss: 5.1423 - ACC: 0.2136 - PPL: 171.6071 - val_loss: 5.1418 - val_ACC: 0.2222 - val_PPL: 171.4247\n",
      "Epoch 89/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1327 - ACC: 0.2149 - PPL: 169.7122\n",
      "Epoch 00089: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 896ms/step - loss: 5.1304 - ACC: 0.2151 - PPL: 169.3422 - val_loss: 5.1585 - val_ACC: 0.2170 - val_PPL: 174.3544\n",
      "Epoch 90/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1361 - ACC: 0.2145 - PPL: 170.4979\n",
      "Epoch 00090: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 897ms/step - loss: 5.1365 - ACC: 0.2145 - PPL: 170.5559 - val_loss: 5.1692 - val_ACC: 0.2128 - val_PPL: 176.2530\n",
      "Epoch 91/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1052 - ACC: 0.2166 - PPL: 165.2903\n",
      "Epoch 00091: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 895ms/step - loss: 5.1051 - ACC: 0.2166 - PPL: 165.2632 - val_loss: 5.1832 - val_ACC: 0.2124 - val_PPL: 178.8861\n",
      "Epoch 92/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1150 - ACC: 0.2154 - PPL: 166.7657\n",
      "Epoch 00092: val_PPL improved from 171.42467 to 168.47321, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 45s 904ms/step - loss: 5.1158 - ACC: 0.2155 - PPL: 166.9013 - val_loss: 5.1241 - val_ACC: 0.2227 - val_PPL: 168.4732\n",
      "Epoch 93/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1128 - ACC: 0.2153 - PPL: 166.3788\n",
      "Epoch 00093: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 898ms/step - loss: 5.1116 - ACC: 0.2155 - PPL: 166.1844 - val_loss: 5.1462 - val_ACC: 0.2225 - val_PPL: 172.3018\n",
      "Epoch 94/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1122 - ACC: 0.2172 - PPL: 166.4933\n",
      "Epoch 00094: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 891ms/step - loss: 5.1121 - ACC: 0.2172 - PPL: 166.4672 - val_loss: 5.1329 - val_ACC: 0.2237 - val_PPL: 169.9630\n",
      "Epoch 95/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1103 - ACC: 0.2157 - PPL: 166.1186\n",
      "Epoch 00095: val_PPL did not improve\n",
      "50/50 [==============================] - 46s 920ms/step - loss: 5.1109 - ACC: 0.2158 - PPL: 166.2195 - val_loss: 5.1351 - val_ACC: 0.2224 - val_PPL: 170.2886\n",
      "Epoch 96/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0890 - ACC: 0.2192 - PPL: 162.5702\n",
      "Epoch 00096: val_PPL improved from 168.47321 to 166.70837, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 45s 896ms/step - loss: 5.0901 - ACC: 0.2191 - PPL: 162.7436 - val_loss: 5.1143 - val_ACC: 0.2241 - val_PPL: 166.7084\n",
      "Epoch 97/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0900 - ACC: 0.2184 - PPL: 162.8279\n",
      "Epoch 00097: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 878ms/step - loss: 5.0916 - ACC: 0.2182 - PPL: 163.0958 - val_loss: 5.1275 - val_ACC: 0.2229 - val_PPL: 168.9958\n",
      "Epoch 98/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0876 - ACC: 0.2183 - PPL: 162.3898\n",
      "Epoch 00098: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 894ms/step - loss: 5.0877 - ACC: 0.2184 - PPL: 162.3910 - val_loss: 5.1166 - val_ACC: 0.2248 - val_PPL: 167.3093\n",
      "Epoch 99/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1027 - ACC: 0.2187 - PPL: 164.8282\n",
      "Epoch 00099: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 884ms/step - loss: 5.1023 - ACC: 0.2187 - PPL: 164.7503 - val_loss: 5.1258 - val_ACC: 0.2225 - val_PPL: 168.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0905 - ACC: 0.2180 - PPL: 162.7309\n",
      "Epoch 00100: val_PPL improved from 166.70837 to 166.46328, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 886ms/step - loss: 5.0906 - ACC: 0.2180 - PPL: 162.7531 - val_loss: 5.1119 - val_ACC: 0.2246 - val_PPL: 166.4633\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(generator=gen_word_word(), \n",
    "                           steps_per_epoch=50, epochs=125,\n",
    "                           callbacks=callback_lists,\n",
    "                           validation_data=gen_word_word(dataset='valid'),\n",
    "                           validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2l21JdFPo3A"
   },
   "outputs": [],
   "source": [
    "history = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "jQLqEZwRPo3M",
    "outputId": "9e0c5672-82b6-4fdb-8f11-739eb0007baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ACC', 'PPL', 'loss', 'val_ACC', 'val_PPL', 'val_loss'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1574705400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.columns)\n",
    "history.loc[:,['PPL','val_PPL']].tail(4600).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI9DyqjoPo3s"
   },
   "outputs": [],
   "source": [
    "# path_model = local_path + 'model/model.keras'\n",
    "# model.save(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_USEcjEPo3x"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "model_restore = load_model(path_model, custom_objects={'ACC':ACC,'PPL': PPL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1129
    },
    "colab_type": "code",
    "id": "9iSl3szGPo37",
    "outputId": "150689c7-d3fb-4935-9184-dfc4ec6ef708",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0887 - ACC: 0.2185 - PPL: 162.5365\n",
      "Epoch 00001: val_PPL did not improve\n",
      "50/50 [==============================] - 48s 959ms/step - loss: 5.0894 - ACC: 0.2186 - PPL: 162.6333 - val_loss: 5.1154 - val_ACC: 0.2261 - val_PPL: 166.7546\n"
     ]
    }
   ],
   "source": [
    "hist = model_restore.fit_generator(generator=gen_word_word(), \n",
    "                           steps_per_epoch=50, epochs=1,\n",
    "                           callbacks=callback_lists,\n",
    "                           validation_data=gen_word_word(dataset='valid'),\n",
    "                           validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "WPNTJojePo4D",
    "outputId": "8b2ce958-73e2-4498-9f01-78d2f5d49a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ACC', 'PPL', 'loss', 'val_ACC', 'val_PPL', 'val_loss'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1574666b70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "print(history.columns)\n",
    "history.loc[:,['PPL','val_PPL']].tail(4600).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SY8V79vOPo4N"
   },
   "outputs": [],
   "source": [
    "def predict_seq(model, preSeq=None, genLen=seq_len, power=1):\n",
    "    \"\"\" Predict a sequence with length genLen.\n",
    "        arg:\n",
    "            model: Keras model used to predict.\n",
    "            preSeq: list. The leading sequence.\n",
    "            genLen: float or np.inf. If power is equal to np.inf, then an argmax will be used. \n",
    "            power: Probility power.\n",
    "    \"\"\"\n",
    "    preSeq = [word_to_id['<SS>']] if preSeq == None else [word_to_id['<SS>']] + preSeq   \n",
    "    pointer = len(preSeq) - 1\n",
    "    \n",
    "    for _ in range(genLen):\n",
    "        inputSeq = np.array([preSeq])\n",
    "        prob = model.predict(inputSeq)[0, pointer, :]\n",
    "        if power==np.inf:\n",
    "            pred = np.argmax(prob)\n",
    "        else:\n",
    "            prob = np.power(prob, power)\n",
    "            prob = prob / np.sum(prob)\n",
    "            pred = np.random.choice(range(voc_size), p=prob)\n",
    "        preSeq.append(pred)\n",
    "        pointer = pointer + 1\n",
    "\n",
    "    return preSeq, ' '.join([id_to_word[id] for id in preSeq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SS> mr. cable says <eos> according to refinancing notably millions profitable directors increased N to N N off the end of september <eos> stock-index arbitrage trading has led as an fall in the <unk> <EE> <EE>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, seq = predict_seq(model, power=1)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character aware model\n",
    "\n",
    "[Character-Aware Neural Language Models -- arxiv-1508.06615 -- AAAI 2016](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "[Ref: Github/jarfo/kchar](https://github.com/jarfo/kchar)\n",
    "\n",
    "\n",
    "![model](https://github.com/stikbuf/Language_Modeling/blob/master/Character%20aware.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKog3W-MPo4c"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'word length distribution, max=19, min=1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_text = [id_to_word[idx] for idx in train_data]\n",
    "valid_data_text = [id_to_word[idx] for idx in valid_data]\n",
    "test_data_text = [id_to_word[idx] for idx in test_data]\n",
    "total_data_text = train_data_text + valid_data_text + test_data_text\n",
    "\n",
    "maxWordLen = max([len(word) for word in total_data_text])\n",
    "maxWordLen += 2 # Start and End character\n",
    "\n",
    "ds = pd.Series([len(word) for word in total_data_text])\n",
    "ds.plot.hist(bins=range(1, maxWordLen))\n",
    "plt.title('word length distribution, max={0}, min={1}'.\n",
    "          format(ds.max(), ds.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars: 51 \n",
      "\n",
      "['q', '8', 'E', '#', '3', 'r', 't', '0', 'n', 'a', 'h', 'p', '<', '6', '$', 'i', 'x', '4', 'N', 'k', '>', 'j', 's', 'm', '7', 'e', '/', '1', 'u', 'b', 'v', ' ', '-', \"'\", 'f', '.', 'o', 'd', 'w', '\\\\', 'y', '5', '&', 'g', 'S', '2', '*', 'z', 'c', '9', 'l'] \n",
      "\n",
      "{0: 'q', 1: '8', 2: 'E', 3: '#', 4: '3', 5: 'r', 6: 't', 7: '0', 8: 'n', 9: 'a', 10: 'h', 11: 'p', 12: '<', 13: '6', 14: '$', 15: 'i', 16: 'x', 17: '4', 18: 'N', 19: 'k', 20: '>', 21: 'j', 22: 's', 23: 'm', 24: '7', 25: 'e', 26: '/', 27: '1', 28: 'u', 29: 'b', 30: 'v', 31: ' ', 32: '-', 33: \"'\", 34: 'f', 35: '.', 36: 'o', 37: 'd', 38: 'w', 39: '\\\\', 40: 'y', 41: '5', 42: '&', 43: 'g', 44: 'S', 45: '2', 46: '*', 47: 'z', 48: 'c', 49: '9', 50: 'l'} \n",
      "\n",
      "{'q': 0, 'E': 2, '#': 3, '3': 4, 'r': 5, '0': 7, 'p': 11, 'x': 16, 'n': 8, 'a': 9, 'h': 10, '8': 1, '<': 12, '6': 13, 'k': 19, '$': 14, 'i': 15, '4': 17, 'N': 18, 'j': 21, '>': 20, '\\\\': 39, 's': 22, '/': 26, '7': 24, 'e': 25, '1': 27, 'v': 30, '5': 41, ' ': 31, '-': 32, \"'\": 33, 'm': 23, 'f': 34, '.': 35, 'o': 36, 'd': 37, 'w': 38, 'y': 40, 't': 6, '&': 42, 'g': 43, 'S': 44, '2': 45, '*': 46, 'z': 47, 'c': 48, 'u': 28, 'b': 29, '9': 49, 'l': 50} \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chars = []\n",
    "for word in total_data_text:\n",
    "    chars.extend(list(word))\n",
    "    \n",
    "ds = pd.Series(chars)\n",
    "ds.value_counts().plot.bar()\n",
    "plt.title('character distribution, total {0} characters(without \\'S\\' and \\'E\\')'.format(len(set(chars))))\n",
    "\n",
    "chars = list(set(chars + ['S'] + ['E'] + [' ']))\n",
    "# 'S' for word leading char, 'E' for word ending char, space for padding\n",
    "id_to_chars = dict(enumerate(chars))\n",
    "chars_to_id = dict((v, k) for k,v in id_to_chars.items())\n",
    "num_chars = len(chars)\n",
    "\n",
    "print('number of chars:', num_chars, '\\n')\n",
    "print(chars, '\\n')\n",
    "print(id_to_chars, '\\n')\n",
    "print(chars_to_id, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 6,\n",
       " 10,\n",
       " 25,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_charId(wordId):\n",
    "    wordString = 'S' + id_to_word[wordId].center(maxWordLen - 2) + 'E'\n",
    "    return [chars_to_id[char] for char in wordString]\n",
    "\n",
    "def wordSeq_charSeq(bWordSeq):\n",
    "    batch, seqLen = bWordSeq.shape\n",
    "    bWordSeq = bWordSeq.ravel()\n",
    "    charSeq = np.array([word_to_charId(wordId) for wordId in bWordSeq])\n",
    "    return charSeq.reshape(batch, seqLen, -1)\n",
    "\n",
    "word_to_charId(word_to_id['the']) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical \n",
    "\n",
    "def gen_char_word(batch_size=128, dataset='train'):\n",
    "    assert dataset in ['train', 'valid', 'test'], 'Dataset must be train or valid or test.'\n",
    "    \n",
    "    dic = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    data = dic[dataset]\n",
    "    \n",
    "    while True:\n",
    "        rnd_idxs = list(range(len(data)-seq_len-1))\n",
    "        random.shuffle(rnd_idxs)\n",
    "        cnt = 0\n",
    "        while cnt < len(rnd_idxs) - batch_size :\n",
    "            X = np.array([[word_to_id['<SS>']] + data[i:i+seq_len] + [word_to_id['<EE>']]\n",
    "                          for i in rnd_idxs[cnt:cnt+batch_size]])\n",
    "            Y = X[:,1:]\n",
    "            X = X[:,:-1]\n",
    "            Y = to_categorical(Y)\n",
    "            X = wordSeq_charSeq(X) \n",
    "            #print(X.shape)\n",
    "            cnt += batch_size\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10002,)\n"
     ]
    }
   ],
   "source": [
    "print(next(gen_char_word(batch_size=1, dataset='train'))[1][0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 20\n",
    "        self.seq_length = seq_len\n",
    "        self.max_word_l = maxWordLen\n",
    "        self.char_vocab_size = num_chars\n",
    "        self.char_vec_size = 15\n",
    "        self.feature_maps = [50,100,150,200,200,200,200]\n",
    "        self.kernels = [1,2,3,4,5,6,7]\n",
    "        self.highway_layers = 2\n",
    "        self.num_lstm_layers = 2\n",
    "        self.rnn_size = 650\n",
    "        self.word_vocab_size = voc_size\n",
    "        self.dropout = 0.5\n",
    "        self.learing_rate = 1\n",
    "        \n",
    "opt = Option()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "![CNN part](https://github.com/stikbuf/Language_Modeling/blob/master/Character%20aware-CNN.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Concatenate, Reshape\n",
    "\n",
    "def CNN(seq_length, length, feature_maps, kernels, x):\n",
    "\n",
    "    concat_input = []\n",
    "    for feature_map, kernel in zip(feature_maps, kernels):\n",
    "        reduced_l = length - kernel + 1\n",
    "        conv = Conv2D(feature_map, (1, kernel), activation='tanh', data_format=\"channels_last\")(x)\n",
    "        maxp = MaxPooling2D((1, reduced_l), data_format=\"channels_last\")(conv)\n",
    "        concat_input.append(maxp)\n",
    "\n",
    "    x = Concatenate()(concat_input)\n",
    "    x = Reshape((seq_length, sum(feature_maps)))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Network  \n",
    "[Srivastava et al.](https://arxiv.org/abs/1505.00387)\n",
    "\n",
    "Input vector is $\\textbf{y}$, then layer output $\\textbf{z}$ is\n",
    "$$\\textbf{z = t} \\odot g(\\textbf{W}_H\\textbf{y}+\\textbf{b}_H) + \\textbf{(1 - t)} \\odot \\textbf{y}$$\n",
    "where \n",
    "$$\\textbf{t} = \\sigma(\\textbf{W}_T\\textbf{y}+\\textbf{b}_T)$$\n",
    "\n",
    "$\\textbf{t}$ is called the\n",
    "*transform gate*, and $(\\textbf{1}−\\textbf{t})$ is called the *carry gate*. \n",
    "Similar to the memory cells in LSTM networks, highway layers allow for training of deep networks by adaptively carrying some dimensions of the input directly to the output. By construction the dimensions of $\\textbf{y}$ and $\\textbf{z}$ have to match, and hence $\\textbf{W}_T$ and $\\textbf{W}_H$ are square matrices.  \n",
    "\n",
    "A keras model is also a keras layer! So you can combine some keras layers to design your own layer. This is useful when combining with TimeDistributed wrapper. [See section 3 in this blog](https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Multiply, Add, Lambda, Input\n",
    "from tensorflow.python.keras.initializers import Constant\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "def Highway(value, nLayers, activation='tanh', gateBias=-3):\n",
    "    dim = K.int_shape(value)[-1]\n",
    "    gateBiasInitalizer = Constant(gateBias)\n",
    "    for i in range(nLayers):\n",
    "        tGate = Dense(units=dim, bias_initializer=gateBiasInitalizer)(value)\n",
    "        tGate = Activation('sigmoid')(tGate)\n",
    "        cGate = Lambda(lambda x: 1.0-x)(tGate) # I do not specify output_shape\n",
    "        transformed = Dense(units=dim, bias_initializer=gateBiasInitalizer)(value)\n",
    "        transformed = Activation(activation)(value)\n",
    "        transformedGate = Multiply()([tGate, transformed])\n",
    "        identityGate = Multiply()([cGate, value])\n",
    "        value = Add()([transformedGate, identityGate])\n",
    "    return value\n",
    "\n",
    "inputs = Input((sum(opt.feature_maps),))\n",
    "HighwayLayer = Model(inputs=inputs, outputs=Highway(inputs, nLayers=opt.highway_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "chars (InputLayer)              (20, 35, 21)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "chars_embedding (Embedding)     (20, 35, 21, 15)     765         chars[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (20, 35, 21, 50)     800         chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (20, 35, 20, 100)    3100        chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (20, 35, 19, 150)    6900        chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (20, 35, 18, 200)    12200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (20, 35, 17, 200)    15200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (20, 35, 16, 200)    18200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (20, 35, 15, 200)    21200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling2D) (20, 35, 1, 50)      0           conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling2D) (20, 35, 1, 100)     0           conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling2D) (20, 35, 1, 150)     0           conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling2D) (20, 35, 1, 200)     0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling2D) (20, 35, 1, 200)     0           conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling2D) (20, 35, 1, 200)     0           conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling2D) (20, 35, 1, 200)     0           conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (20, 35, 1, 1100)    0           max_pooling2d_57[0][0]           \n",
      "                                                                 max_pooling2d_58[0][0]           \n",
      "                                                                 max_pooling2d_59[0][0]           \n",
      "                                                                 max_pooling2d_60[0][0]           \n",
      "                                                                 max_pooling2d_61[0][0]           \n",
      "                                                                 max_pooling2d_62[0][0]           \n",
      "                                                                 max_pooling2d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (20, 35, 1100)       0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (20, 35, 1100)       4400        reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (20, 35, 1100)       2422200     batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (20, 35, 650)        4552600     time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (20, 35, 650)        0           lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (20, 35, 650)        3382600     dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (20, 35, 650)        0           lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (20, 35, 10002)      6511302     dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 16,951,467\n",
      "Trainable params: 16,949,267\n",
      "Non-trainable params: 2,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization, TimeDistributed\n",
    "#from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "chars = Input(batch_shape=(opt.batch_size, opt.seq_length, opt.max_word_l), name='chars')\n",
    "chars_embedding = Embedding(opt.char_vocab_size, opt.char_vec_size, name='chars_embedding')(chars)\n",
    "cnn = CNN(opt.seq_length, opt.max_word_l, opt.feature_maps, opt.kernels, chars_embedding)\n",
    "x = cnn\n",
    "inputs = chars\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = TimeDistributed(HighwayLayer)(x)\n",
    "\n",
    "for l in range(opt.num_lstm_layers):\n",
    "    x = LSTM(opt.rnn_size, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, stateful=True)(x)\n",
    "\n",
    "    if opt.dropout > 0:\n",
    "        x = Dropout(opt.dropout)(x)\n",
    "        \n",
    "output = TimeDistributed(Dense(opt.word_vocab_size, activation='softmax'))(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "#optimizer = SGD(lr=opt.learning_rate, clipnorm=opt.max_grad_norm, scale=float(opt.seq_length))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras - Character-Aware Neural Language Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
