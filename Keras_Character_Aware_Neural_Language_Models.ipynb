{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/stikbuf/Language_Modeling/blob/master/Keras_Character_Aware_Neural_Language_Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_0CMzmyQXoy"
   },
   "source": [
    "## Configure the cloud environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QB7JyNTfQfKF"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2539
    },
    "colab_type": "code",
    "id": "oTB-axrvQiZU",
    "outputId": "e2997ab7-5a01-4406-d696-aae0c2c981aa"
   },
   "outputs": [],
   "source": [
    "# Install a Drive FUSE wrapper.\n",
    "# https://github.com/astrada/google-drive-ocamlfuse\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "\n",
    "# Generate auth tokens for Colab\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "\n",
    "# Generate creds for the Drive FUSE library.\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zVhUtsJjQqy5",
    "outputId": "fbf684be-5cab-471f-ed07-2cc929cc4e7d"
   },
   "outputs": [],
   "source": [
    "# If you got a \"Transport endpoint is not connected.\" error. Please run this line first to unmount the drive.\n",
    "# See https://stackoverflow.com/questions/49588113/google-colab-script-throws-transport-endpoint-is-not-connected?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "!fusermount -u drive\n",
    "\n",
    "# Create a directory and mount Google Drive using that directory.\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "a = !ls drive/\n",
    "print('Files in Drive:', a)\n",
    "assert a!=[], 'Drive should not be empty!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "bvfVOCzkRErx",
    "outputId": "2b10a93a-4be4-457f-ff93-749ace078ab4"
   },
   "outputs": [],
   "source": [
    "local_path='./drive/share_with_me/AI/Character-aware_LM/'\n",
    "#local_path='./'\n",
    "import sys\n",
    "sys.path.append(local_path)\n",
    "!ls './drive/share_with_me/AI/Character-aware_LM/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TbWRANsEQr6U",
    "outputId": "fbedffd8-9029-4ca3-c73c-b87c6a74bfe6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#assert tf.test.gpu_device_name() != '', \"GPU not avaliable!\"\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUldvgY1RH0S"
   },
   "source": [
    "## Load data (Penn Tree bank -- PTB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NgfjuJbPoz9"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib  \n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from reader import ptb_raw_data, ptb_producer # by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dpHEJHNnPo0U",
    "outputId": "78431c7b-a03d-4609-a381-9d7aa5b96632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data, test_data, word_to_id = ptb_raw_data(local_path + 'data') # tokens\n",
    "id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
    "voc_size = len(id_to_word)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 929589, Valid data size: 73760, Test data size: 82430\n",
      "\n",
      "train/val/test_data is a list, some elements in train_data is [9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983]\n"
     ]
    }
   ],
   "source": [
    "print('Train data size: {0}, Valid data size: {1}, Test data size: {2}\\n'.\n",
    "      format(len(train_data), len(valid_data), len(test_data)))\n",
    "print('train/val/test_data is a list, some elements in train_data is', train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oZ8sUleQSoWQ",
    "outputId": "c4796a76-4fe3-48a3-f62a-bef541f937b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[voc_size]='<SS>' # Add start word token '<SS>'\n",
    "id_to_word[voc_size+1]='<EE>' # Add end word token '<EE>'\n",
    "word_to_id = dict((v, k) for k, v in id_to_word.items())\n",
    "voc_size = len(id_to_word)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "zj8IWqJ8Po0h",
    "outputId": "5e9fded6-d6d9-44f1-dfa6-9fdece6f2df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id\n",
      "the     0\n",
      "<unk>   1\n",
      "<eos>   2\n",
      "N       3\n",
      "of      4\n",
      "              id\n",
      "ssangyong   9997\n",
      "swapo       9998\n",
      "wachter     9999\n",
      "<SS>       10000\n",
      "<EE>       10001\n"
     ]
    }
   ],
   "source": [
    "word_id = pd.DataFrame.from_dict(word_to_id, orient='index').sort_values(by=0, ascending=True)\n",
    "word_id.columns = ['id']\n",
    "print(word_id.head())\n",
    "print(word_id.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "uxPQPQqMPo0t",
    "outputId": "946af7f2-7906-4fa3-f2bc-d1d7bffe5428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word\n",
      "0    the\n",
      "1  <unk>\n",
      "2  <eos>\n",
      "3      N\n",
      "4     of\n",
      "            word\n",
      "9997   ssangyong\n",
      "9998       swapo\n",
      "9999     wachter\n",
      "10000       <SS>\n",
      "10001       <EE>\n"
     ]
    }
   ],
   "source": [
    "id_word = pd.DataFrame.from_dict(id_to_word, orient='index')\n",
    "id_word.columns = ['word']\n",
    "print(id_word.head())\n",
    "print(id_word.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Hz5IRBM6Po04",
    "outputId": "3d885c2d-15f3-4176-e943-1ae2ed489515"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk> N years old will join the board as a nonexecutive director nov. N <eos> mr. <unk> is chairman of <unk> n.v. the dutch'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([id_to_word[id] for id in train_data[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDwfc2jpPo1D"
   },
   "source": [
    "# RNN baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPQMjhyePo1F"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical \n",
    "\n",
    "def gen_word_word(batch_size=128, dataset='train'):\n",
    "    assert dataset in ['train', 'valid', 'test'], 'Dataset must be train or valid or test.'\n",
    "    \n",
    "    dic = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    data = dic[dataset]\n",
    "    \n",
    "    while True:\n",
    "        rnd_idxs = list(range(len(data)-seq_len-1))\n",
    "        random.shuffle(rnd_idxs)\n",
    "        cnt = 0\n",
    "        while cnt < len(rnd_idxs) - batch_size :\n",
    "            X = np.array([[word_to_id['<SS>']] + data[i:i+seq_len] + [word_to_id['<EE>']]\n",
    "                          for i in rnd_idxs[cnt:cnt+batch_size]])\n",
    "            Y = X[:,1:]\n",
    "            X = X[:,:-1]\n",
    "            Y = to_categorical(Y)\n",
    "            #print(X.shape)\n",
    "            cnt += batch_size\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JL6Za0iNPo1O"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import GRU, Dense, Embedding, InputLayer, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout between layers, see [Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt6IvftHPo1Y"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "\n",
    "model.add(Embedding(input_dim=voc_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='inputEmbedding'))\n",
    "model.add(GRU(units=128, return_sequences=True))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(GRU(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(voc_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Evcq5fYye8jt"
   },
   "outputs": [],
   "source": [
    "# perplexity\n",
    "def PPL(y_true, y_pred):\n",
    "    return tf.exp(tf.reduce_mean(tf.keras.backend.categorical_crossentropy(y_true, y_pred)))\n",
    "\n",
    "def ACC(y_true, y_pred):\n",
    "    ACC = tf.equal(tf.argmax(y_true, axis = 2), \n",
    "                   tf.argmax(y_pred, axis = 2))\n",
    "    ACC = tf.cast(ACC, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRoe-64bZAWz"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[ACC, PPL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "colab_type": "code",
    "id": "efPbWtAJPo2P",
    "outputId": "c9bf68c6-3436-44f3-e4e4-04b33a82dfd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputEmbedding (Embedding)   (None, None, 128)         1280256   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         98688     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 64)          37056     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 10002)       650130    \n",
      "=================================================================\n",
      "Total params: 2,066,130\n",
      "Trainable params: 2,066,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import os\n",
    "if not os.path.exists(local_path + 'model/'):\n",
    "    os.mkdir(local_path + 'model/')\n",
    "\n",
    "path_model = local_path + 'model/model.keras'    \n",
    "tensorboard = TensorBoard(log_dir='log')\n",
    "checkpoint = ModelCheckpoint(filepath=path_model, verbose=1,\n",
    "                             monitor='val_PPL',mode='min' ,save_best_only='True')\n",
    "# path_model = local_path + 'model/model.keras'\n",
    "# model.save(path_model)\n",
    "\n",
    "callback_lists=[tensorboard,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3677
    },
    "colab_type": "code",
    "id": "7rWihlHePo2Z",
    "outputId": "19ef2c40-5173-42d2-e4ff-d43ff273cd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 7.4025 - ACC: 0.0299 - PPL: 2238.6179\n",
      "Epoch 00001: val_PPL improved from inf to 729.31307, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 55s 1s/step - loss: 7.3870 - ACC: 0.0305 - PPL: 2208.9620 - val_loss: 6.5916 - val_ACC: 0.0547 - val_PPL: 729.3131\n",
      "Epoch 2/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.5222 - ACC: 0.0527 - PPL: 680.8545\n",
      "Epoch 00002: val_PPL improved from 729.31307 to 656.51662, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.5233 - ACC: 0.0528 - PPL: 681.6252 - val_loss: 6.4862 - val_ACC: 0.0551 - val_PPL: 656.5166\n",
      "Epoch 3/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.5067 - ACC: 0.0540 - PPL: 670.2737\n",
      "Epoch 00003: val_PPL improved from 656.51662 to 649.24152, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 52s 1s/step - loss: 6.5061 - ACC: 0.0542 - PPL: 669.8924 - val_loss: 6.4749 - val_ACC: 0.0610 - val_PPL: 649.2415\n",
      "Epoch 4/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.4499 - ACC: 0.0648 - PPL: 633.3455\n",
      "Epoch 00004: val_PPL improved from 649.24152 to 604.99419, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 971ms/step - loss: 6.4498 - ACC: 0.0649 - PPL: 633.2387 - val_loss: 6.4041 - val_ACC: 0.0653 - val_PPL: 604.9942\n",
      "Epoch 5/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3868 - ACC: 0.0735 - PPL: 594.6345\n",
      "Epoch 00005: val_PPL improved from 604.99419 to 562.52958, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.3857 - ACC: 0.0736 - PPL: 593.9719 - val_loss: 6.3311 - val_ACC: 0.0865 - val_PPL: 562.5296\n",
      "Epoch 6/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3105 - ACC: 0.0879 - PPL: 550.8956\n",
      "Epoch 00006: val_PPL improved from 562.52958 to 543.89324, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.3095 - ACC: 0.0880 - PPL: 550.3092 - val_loss: 6.2976 - val_ACC: 0.0960 - val_PPL: 543.8932\n",
      "Epoch 7/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.2349 - ACC: 0.1018 - PPL: 510.7661\n",
      "Epoch 00007: val_PPL improved from 543.89324 to 487.47160, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 974ms/step - loss: 6.2331 - ACC: 0.1020 - PPL: 509.8584 - val_loss: 6.1879 - val_ACC: 0.1118 - val_PPL: 487.4716\n",
      "Epoch 8/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1627 - ACC: 0.1119 - PPL: 475.4521\n",
      "Epoch 00008: val_PPL improved from 487.47160 to 456.62706, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.1609 - ACC: 0.1121 - PPL: 474.6371 - val_loss: 6.1225 - val_ACC: 0.1177 - val_PPL: 456.6271\n",
      "Epoch 9/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0888 - ACC: 0.1191 - PPL: 441.5484\n",
      "Epoch 00009: val_PPL improved from 456.62706 to 423.12172, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.0874 - ACC: 0.1192 - PPL: 440.9254 - val_loss: 6.0463 - val_ACC: 0.1232 - val_PPL: 423.1217\n",
      "Epoch 10/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0156 - ACC: 0.1255 - PPL: 410.4352\n",
      "Epoch 00010: val_PPL improved from 423.12172 to 397.22348, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 6.0170 - ACC: 0.1255 - PPL: 410.9762 - val_loss: 5.9831 - val_ACC: 0.1267 - val_PPL: 397.2235\n",
      "Epoch 11/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9734 - ACC: 0.1313 - PPL: 393.3070\n",
      "Epoch 00011: val_PPL improved from 397.22348 to 386.95126, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.9742 - ACC: 0.1312 - PPL: 393.6298 - val_loss: 5.9565 - val_ACC: 0.1302 - val_PPL: 386.9513\n",
      "Epoch 12/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9391 - ACC: 0.1355 - PPL: 380.3861\n",
      "Epoch 00012: val_PPL improved from 386.95126 to 375.40058, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.9404 - ACC: 0.1353 - PPL: 380.8729 - val_loss: 5.9265 - val_ACC: 0.1407 - val_PPL: 375.4006\n",
      "Epoch 13/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8889 - ACC: 0.1390 - PPL: 361.5832\n",
      "Epoch 00013: val_PPL improved from 375.40058 to 348.37696, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.8874 - ACC: 0.1389 - PPL: 361.0691 - val_loss: 5.8519 - val_ACC: 0.1486 - val_PPL: 348.3770\n",
      "Epoch 14/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8501 - ACC: 0.1447 - PPL: 347.9056\n",
      "Epoch 00014: val_PPL improved from 348.37696 to 333.67055, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.8486 - ACC: 0.1448 - PPL: 347.3916 - val_loss: 5.8081 - val_ACC: 0.1517 - val_PPL: 333.6706\n",
      "Epoch 15/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8114 - ACC: 0.1471 - PPL: 334.4995\n",
      "Epoch 00015: val_PPL improved from 333.67055 to 325.36930, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 985ms/step - loss: 5.8112 - ACC: 0.1472 - PPL: 334.4446 - val_loss: 5.7832 - val_ACC: 0.1513 - val_PPL: 325.3693\n",
      "Epoch 16/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7789 - ACC: 0.1500 - PPL: 323.8882\n",
      "Epoch 00016: val_PPL improved from 325.36930 to 314.00920, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.7804 - ACC: 0.1500 - PPL: 324.3939 - val_loss: 5.7475 - val_ACC: 0.1568 - val_PPL: 314.0092\n",
      "Epoch 17/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7537 - ACC: 0.1519 - PPL: 315.8327\n",
      "Epoch 00017: val_PPL improved from 314.00920 to 308.17599, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.7532 - ACC: 0.1521 - PPL: 315.6607 - val_loss: 5.7289 - val_ACC: 0.1604 - val_PPL: 308.1760\n",
      "Epoch 18/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7405 - ACC: 0.1536 - PPL: 311.8603\n",
      "Epoch 00018: val_PPL improved from 308.17599 to 301.13895, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.7414 - ACC: 0.1535 - PPL: 312.1449 - val_loss: 5.7051 - val_ACC: 0.1630 - val_PPL: 301.1389\n",
      "Epoch 19/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6998 - ACC: 0.1574 - PPL: 299.1991\n",
      "Epoch 00019: val_PPL improved from 301.13895 to 298.01215, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.7018 - ACC: 0.1573 - PPL: 299.8010 - val_loss: 5.6959 - val_ACC: 0.1607 - val_PPL: 298.0122\n",
      "Epoch 20/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6859 - ACC: 0.1588 - PPL: 295.3874\n",
      "Epoch 00020: val_PPL improved from 298.01215 to 285.81103, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.6845 - ACC: 0.1588 - PPL: 294.9730 - val_loss: 5.6535 - val_ACC: 0.1650 - val_PPL: 285.8110\n",
      "Epoch 21/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6441 - ACC: 0.1607 - PPL: 283.1733\n",
      "Epoch 00021: val_PPL improved from 285.81103 to 285.39072, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 994ms/step - loss: 5.6414 - ACC: 0.1609 - PPL: 282.4456 - val_loss: 5.6519 - val_ACC: 0.1674 - val_PPL: 285.3907\n",
      "Epoch 22/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6282 - ACC: 0.1644 - PPL: 278.7390\n",
      "Epoch 00022: val_PPL improved from 285.39072 to 272.89878, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.6290 - ACC: 0.1644 - PPL: 278.9714 - val_loss: 5.6072 - val_ACC: 0.1751 - val_PPL: 272.8988\n",
      "Epoch 23/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6140 - ACC: 0.1648 - PPL: 274.6704\n",
      "Epoch 00023: val_PPL did not improve\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.6144 - ACC: 0.1648 - PPL: 274.7862 - val_loss: 5.6206 - val_ACC: 0.1667 - val_PPL: 276.8625\n",
      "Epoch 24/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5912 - ACC: 0.1686 - PPL: 268.5093\n",
      "Epoch 00024: val_PPL improved from 272.89878 to 264.91221, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.5914 - ACC: 0.1685 - PPL: 268.5535 - val_loss: 5.5774 - val_ACC: 0.1753 - val_PPL: 264.9122\n",
      "Epoch 25/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5866 - ACC: 0.1677 - PPL: 267.2568\n",
      "Epoch 00025: val_PPL improved from 264.91221 to 262.39913, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 958ms/step - loss: 5.5886 - ACC: 0.1673 - PPL: 267.8107 - val_loss: 5.5672 - val_ACC: 0.1775 - val_PPL: 262.3991\n",
      "Epoch 26/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5698 - ACC: 0.1702 - PPL: 262.7644\n",
      "Epoch 00026: val_PPL improved from 262.39913 to 257.71197, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.5693 - ACC: 0.1701 - PPL: 262.6315 - val_loss: 5.5499 - val_ACC: 0.1781 - val_PPL: 257.7120\n",
      "Epoch 27/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5527 - ACC: 0.1712 - PPL: 258.5783\n",
      "Epoch 00027: val_PPL improved from 257.71197 to 251.61956, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.5517 - ACC: 0.1714 - PPL: 258.3164 - val_loss: 5.5260 - val_ACC: 0.1817 - val_PPL: 251.6196\n",
      "Epoch 28/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5348 - ACC: 0.1733 - PPL: 254.0451\n",
      "Epoch 00028: val_PPL improved from 251.61956 to 248.79050, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 52s 1s/step - loss: 5.5351 - ACC: 0.1731 - PPL: 254.1087 - val_loss: 5.5148 - val_ACC: 0.1807 - val_PPL: 248.7905\n",
      "Epoch 29/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5199 - ACC: 0.1734 - PPL: 249.9035\n",
      "Epoch 00029: val_PPL improved from 248.79050 to 244.28281, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.5190 - ACC: 0.1734 - PPL: 249.6854 - val_loss: 5.4957 - val_ACC: 0.1851 - val_PPL: 244.2828\n",
      "Epoch 30/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4971 - ACC: 0.1771 - PPL: 244.3847\n",
      "Epoch 00030: val_PPL improved from 244.28281 to 240.53591, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4966 - ACC: 0.1770 - PPL: 244.2578 - val_loss: 5.4809 - val_ACC: 0.1835 - val_PPL: 240.5359\n",
      "Epoch 31/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5011 - ACC: 0.1768 - PPL: 245.4008\n",
      "Epoch 00031: val_PPL did not improve\n",
      "50/50 [==============================] - 50s 992ms/step - loss: 5.4983 - ACC: 0.1770 - PPL: 244.7443 - val_loss: 5.4807 - val_ACC: 0.1853 - val_PPL: 240.6217\n",
      "Epoch 32/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4852 - ACC: 0.1770 - PPL: 241.5776\n",
      "Epoch 00032: val_PPL improved from 240.53591 to 239.42074, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 49s 973ms/step - loss: 5.4861 - ACC: 0.1771 - PPL: 241.7754 - val_loss: 5.4760 - val_ACC: 0.1814 - val_PPL: 239.4207\n",
      "Epoch 33/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4726 - ACC: 0.1805 - PPL: 238.7435\n",
      "Epoch 00033: val_PPL improved from 239.42074 to 234.69388, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 965ms/step - loss: 5.4706 - ACC: 0.1806 - PPL: 238.2781 - val_loss: 5.4563 - val_ACC: 0.1878 - val_PPL: 234.6939\n",
      "Epoch 34/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4679 - ACC: 0.1803 - PPL: 237.4297\n",
      "Epoch 00034: val_PPL improved from 234.69388 to 233.52022, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4677 - ACC: 0.1803 - PPL: 237.3770 - val_loss: 5.4509 - val_ACC: 0.1901 - val_PPL: 233.5202\n",
      "Epoch 35/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4459 - ACC: 0.1814 - PPL: 232.3040\n",
      "Epoch 00035: val_PPL improved from 233.52022 to 228.76066, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 52s 1s/step - loss: 5.4471 - ACC: 0.1814 - PPL: 232.5757 - val_loss: 5.4304 - val_ACC: 0.1914 - val_PPL: 228.7607\n",
      "Epoch 36/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4395 - ACC: 0.1817 - PPL: 230.7606\n",
      "Epoch 00036: val_PPL improved from 228.76066 to 225.69043, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4415 - ACC: 0.1815 - PPL: 231.2165 - val_loss: 5.4155 - val_ACC: 0.1929 - val_PPL: 225.6904\n",
      "Epoch 37/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4158 - ACC: 0.1850 - PPL: 225.4565\n",
      "Epoch 00037: val_PPL did not improve\n",
      "50/50 [==============================] - 49s 990ms/step - loss: 5.4141 - ACC: 0.1851 - PPL: 225.0778 - val_loss: 5.4335 - val_ACC: 0.1887 - val_PPL: 229.3768\n",
      "Epoch 38/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4302 - ACC: 0.1842 - PPL: 228.5462\n",
      "Epoch 00038: val_PPL improved from 225.69043 to 224.76569, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.4296 - ACC: 0.1840 - PPL: 228.4052 - val_loss: 5.4122 - val_ACC: 0.1920 - val_PPL: 224.7657\n",
      "Epoch 39/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.4064 - ACC: 0.1851 - PPL: 223.2234\n",
      "Epoch 00039: val_PPL did not improve\n",
      "50/50 [==============================] - 49s 984ms/step - loss: 5.4048 - ACC: 0.1850 - PPL: 222.8637 - val_loss: 5.4252 - val_ACC: 0.1890 - val_PPL: 227.5504\n",
      "Epoch 40/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3942 - ACC: 0.1871 - PPL: 220.5582\n",
      "Epoch 00040: val_PPL improved from 224.76569 to 217.70461, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3949 - ACC: 0.1871 - PPL: 220.7048 - val_loss: 5.3811 - val_ACC: 0.1910 - val_PPL: 217.7046\n",
      "Epoch 41/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3816 - ACC: 0.1883 - PPL: 217.7992\n",
      "Epoch 00041: val_PPL improved from 217.70461 to 216.63400, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.3808 - ACC: 0.1884 - PPL: 217.6164 - val_loss: 5.3757 - val_ACC: 0.1944 - val_PPL: 216.6340\n",
      "Epoch 42/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3697 - ACC: 0.1892 - PPL: 215.1600\n",
      "Epoch 00042: val_PPL improved from 216.63400 to 216.04502, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 968ms/step - loss: 5.3716 - ACC: 0.1889 - PPL: 215.5753 - val_loss: 5.3736 - val_ACC: 0.2002 - val_PPL: 216.0450\n",
      "Epoch 43/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3537 - ACC: 0.1908 - PPL: 211.9230\n",
      "Epoch 00043: val_PPL improved from 216.04502 to 214.37846, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3554 - ACC: 0.1905 - PPL: 212.2878 - val_loss: 5.3659 - val_ACC: 0.1958 - val_PPL: 214.3785\n",
      "Epoch 44/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3676 - ACC: 0.1899 - PPL: 214.6995\n",
      "Epoch 00044: val_PPL improved from 214.37846 to 211.77059, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3680 - ACC: 0.1899 - PPL: 214.7734 - val_loss: 5.3527 - val_ACC: 0.1950 - val_PPL: 211.7706\n",
      "Epoch 45/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3477 - ACC: 0.1909 - PPL: 210.4896\n",
      "Epoch 00045: val_PPL improved from 211.77059 to 206.68730, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3473 - ACC: 0.1909 - PPL: 210.4060 - val_loss: 5.3287 - val_ACC: 0.2024 - val_PPL: 206.6873\n",
      "Epoch 46/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3382 - ACC: 0.1921 - PPL: 208.5513\n",
      "Epoch 00046: val_PPL did not improve\n",
      "50/50 [==============================] - 51s 1s/step - loss: 5.3389 - ACC: 0.1920 - PPL: 208.6750 - val_loss: 5.3356 - val_ACC: 0.2018 - val_PPL: 208.1482\n",
      "Epoch 47/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3463 - ACC: 0.1933 - PPL: 210.1937\n",
      "Epoch 00047: val_PPL did not improve\n",
      "50/50 [==============================] - 50s 1s/step - loss: 5.3460 - ACC: 0.1933 - PPL: 210.1287 - val_loss: 5.3417 - val_ACC: 0.2004 - val_PPL: 209.4001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3376 - ACC: 0.1931 - PPL: 208.4806\n",
      "Epoch 00048: val_PPL improved from 206.68730 to 203.56581, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 48s 957ms/step - loss: 5.3391 - ACC: 0.1930 - PPL: 208.8041 - val_loss: 5.3132 - val_ACC: 0.2039 - val_PPL: 203.5658\n",
      "Epoch 49/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3089 - ACC: 0.1957 - PPL: 202.6321\n",
      "Epoch 00049: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 879ms/step - loss: 5.3088 - ACC: 0.1956 - PPL: 202.5901 - val_loss: 5.3311 - val_ACC: 0.1997 - val_PPL: 207.3809\n",
      "Epoch 50/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.3226 - ACC: 0.1943 - PPL: 205.4597\n",
      "Epoch 00050: val_PPL improved from 203.56581 to 197.65276, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 43s 852ms/step - loss: 5.3217 - ACC: 0.1945 - PPL: 205.2664 - val_loss: 5.2845 - val_ACC: 0.2090 - val_PPL: 197.6528\n",
      "Epoch 51/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2976 - ACC: 0.1970 - PPL: 200.4175\n",
      "Epoch 00051: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 868ms/step - loss: 5.2967 - ACC: 0.1970 - PPL: 200.2395 - val_loss: 5.2857 - val_ACC: 0.2072 - val_PPL: 197.8833\n",
      "Epoch 52/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2983 - ACC: 0.1985 - PPL: 200.4826\n",
      "Epoch 00052: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 886ms/step - loss: 5.2987 - ACC: 0.1985 - PPL: 200.5415 - val_loss: 5.2907 - val_ACC: 0.2077 - val_PPL: 198.9466\n",
      "Epoch 53/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2936 - ACC: 0.1980 - PPL: 199.4159\n",
      "Epoch 00053: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 858ms/step - loss: 5.2931 - ACC: 0.1980 - PPL: 199.2994 - val_loss: 5.2986 - val_ACC: 0.2053 - val_PPL: 200.4542\n",
      "Epoch 54/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2910 - ACC: 0.1970 - PPL: 198.8535\n",
      "Epoch 00054: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 858ms/step - loss: 5.2919 - ACC: 0.1969 - PPL: 199.0458 - val_loss: 5.3025 - val_ACC: 0.2075 - val_PPL: 201.2577\n",
      "Epoch 55/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2814 - ACC: 0.2001 - PPL: 197.0094\n",
      "Epoch 00055: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 861ms/step - loss: 5.2805 - ACC: 0.2002 - PPL: 196.8265 - val_loss: 5.2875 - val_ACC: 0.2037 - val_PPL: 198.3650\n",
      "Epoch 56/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2769 - ACC: 0.1986 - PPL: 196.2050\n",
      "Epoch 00056: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 870ms/step - loss: 5.2766 - ACC: 0.1986 - PPL: 196.1556 - val_loss: 5.2944 - val_ACC: 0.2054 - val_PPL: 199.5631\n",
      "Epoch 57/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2737 - ACC: 0.1995 - PPL: 195.5004\n",
      "Epoch 00057: val_PPL improved from 197.65276 to 195.99028, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 872ms/step - loss: 5.2737 - ACC: 0.1995 - PPL: 195.4971 - val_loss: 5.2755 - val_ACC: 0.2109 - val_PPL: 195.9903\n",
      "Epoch 58/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2429 - ACC: 0.2027 - PPL: 189.8190\n",
      "Epoch 00058: val_PPL did not improve\n",
      "50/50 [==============================] - 42s 837ms/step - loss: 5.2420 - ACC: 0.2028 - PPL: 189.6365 - val_loss: 5.2829 - val_ACC: 0.2050 - val_PPL: 197.4012\n",
      "Epoch 59/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2422 - ACC: 0.2026 - PPL: 189.4378\n",
      "Epoch 00059: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 852ms/step - loss: 5.2425 - ACC: 0.2024 - PPL: 189.4854 - val_loss: 5.2771 - val_ACC: 0.2021 - val_PPL: 196.1804\n",
      "Epoch 60/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2599 - ACC: 0.2026 - PPL: 192.8499\n",
      "Epoch 00060: val_PPL improved from 195.99028 to 189.20414, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 43s 853ms/step - loss: 5.2599 - ACC: 0.2027 - PPL: 192.8379 - val_loss: 5.2401 - val_ACC: 0.2139 - val_PPL: 189.2041\n",
      "Epoch 61/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2587 - ACC: 0.2025 - PPL: 192.7077\n",
      "Epoch 00061: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 885ms/step - loss: 5.2586 - ACC: 0.2025 - PPL: 192.6800 - val_loss: 5.2904 - val_ACC: 0.2010 - val_PPL: 198.8837\n",
      "Epoch 62/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2225 - ACC: 0.2059 - PPL: 185.7580\n",
      "Epoch 00062: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 882ms/step - loss: 5.2237 - ACC: 0.2058 - PPL: 185.9833 - val_loss: 5.2549 - val_ACC: 0.2116 - val_PPL: 191.8594\n",
      "Epoch 63/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2287 - ACC: 0.2046 - PPL: 186.7993\n",
      "Epoch 00063: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 865ms/step - loss: 5.2280 - ACC: 0.2046 - PPL: 186.6704 - val_loss: 5.2444 - val_ACC: 0.2129 - val_PPL: 189.9324\n",
      "Epoch 64/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2229 - ACC: 0.2046 - PPL: 185.9584\n",
      "Epoch 00064: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 863ms/step - loss: 5.2222 - ACC: 0.2045 - PPL: 185.8074 - val_loss: 5.2594 - val_ACC: 0.2096 - val_PPL: 192.6372\n",
      "Epoch 65/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2206 - ACC: 0.2059 - PPL: 185.5133\n",
      "Epoch 00065: val_PPL improved from 189.20414 to 181.13306, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 881ms/step - loss: 5.2197 - ACC: 0.2060 - PPL: 185.3405 - val_loss: 5.1968 - val_ACC: 0.2191 - val_PPL: 181.1331\n",
      "Epoch 66/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2220 - ACC: 0.2055 - PPL: 185.6233\n",
      "Epoch 00066: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 866ms/step - loss: 5.2224 - ACC: 0.2055 - PPL: 185.6783 - val_loss: 5.2178 - val_ACC: 0.2170 - val_PPL: 184.8569\n",
      "Epoch 67/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2159 - ACC: 0.2075 - PPL: 184.6417\n",
      "Epoch 00067: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 878ms/step - loss: 5.2162 - ACC: 0.2075 - PPL: 184.7031 - val_loss: 5.2194 - val_ACC: 0.2094 - val_PPL: 185.1529\n",
      "Epoch 68/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2086 - ACC: 0.2064 - PPL: 183.2449\n",
      "Epoch 00068: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 887ms/step - loss: 5.2084 - ACC: 0.2064 - PPL: 183.2051 - val_loss: 5.2230 - val_ACC: 0.2130 - val_PPL: 185.9379\n",
      "Epoch 69/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2056 - ACC: 0.2055 - PPL: 182.5848\n",
      "Epoch 00069: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 857ms/step - loss: 5.2065 - ACC: 0.2057 - PPL: 182.7500 - val_loss: 5.2227 - val_ACC: 0.2133 - val_PPL: 185.7875\n",
      "Epoch 70/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.2143 - ACC: 0.2052 - PPL: 184.3099\n",
      "Epoch 00070: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 901ms/step - loss: 5.2159 - ACC: 0.2052 - PPL: 184.6133 - val_loss: 5.2040 - val_ACC: 0.2133 - val_PPL: 182.3497\n",
      "Epoch 71/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1867 - ACC: 0.2094 - PPL: 179.1089\n",
      "Epoch 00071: val_PPL improved from 181.13306 to 178.13573, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 888ms/step - loss: 5.1881 - ACC: 0.2093 - PPL: 179.3580 - val_loss: 5.1811 - val_ACC: 0.2202 - val_PPL: 178.1357\n",
      "Epoch 72/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1775 - ACC: 0.2091 - PPL: 177.6599\n",
      "Epoch 00072: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 867ms/step - loss: 5.1765 - ACC: 0.2092 - PPL: 177.4871 - val_loss: 5.2066 - val_ACC: 0.2173 - val_PPL: 182.8368\n",
      "Epoch 73/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1861 - ACC: 0.2096 - PPL: 179.1604\n",
      "Epoch 00073: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 863ms/step - loss: 5.1878 - ACC: 0.2095 - PPL: 179.4650 - val_loss: 5.2073 - val_ACC: 0.2149 - val_PPL: 183.1281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1701 - ACC: 0.2105 - PPL: 176.2739\n",
      "Epoch 00074: val_PPL improved from 178.13573 to 176.60325, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 42s 849ms/step - loss: 5.1714 - ACC: 0.2102 - PPL: 176.5084 - val_loss: 5.1722 - val_ACC: 0.2181 - val_PPL: 176.6033\n",
      "Epoch 75/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1834 - ACC: 0.2090 - PPL: 178.6776\n",
      "Epoch 00075: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 896ms/step - loss: 5.1865 - ACC: 0.2085 - PPL: 179.2699 - val_loss: 5.2059 - val_ACC: 0.2142 - val_PPL: 182.8677\n",
      "Epoch 76/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1875 - ACC: 0.2086 - PPL: 179.4254\n",
      "Epoch 00076: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 856ms/step - loss: 5.1876 - ACC: 0.2086 - PPL: 179.4425 - val_loss: 5.1926 - val_ACC: 0.2175 - val_PPL: 180.5738\n",
      "Epoch 77/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1857 - ACC: 0.2081 - PPL: 178.9215\n",
      "Epoch 00077: val_PPL improved from 176.60325 to 174.83749, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 45s 895ms/step - loss: 5.1845 - ACC: 0.2082 - PPL: 178.6979 - val_loss: 5.1617 - val_ACC: 0.2213 - val_PPL: 174.8375\n",
      "Epoch 78/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1729 - ACC: 0.2096 - PPL: 176.8041\n",
      "Epoch 00078: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 900ms/step - loss: 5.1724 - ACC: 0.2096 - PPL: 176.7159 - val_loss: 5.1841 - val_ACC: 0.2183 - val_PPL: 179.0569\n",
      "Epoch 79/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1538 - ACC: 0.2111 - PPL: 173.6284\n",
      "Epoch 00079: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 867ms/step - loss: 5.1541 - ACC: 0.2111 - PPL: 173.6729 - val_loss: 5.1707 - val_ACC: 0.2190 - val_PPL: 176.4778\n",
      "Epoch 80/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1438 - ACC: 0.2123 - PPL: 171.7268\n",
      "Epoch 00080: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 880ms/step - loss: 5.1430 - ACC: 0.2125 - PPL: 171.5767 - val_loss: 5.1704 - val_ACC: 0.2175 - val_PPL: 176.2964\n",
      "Epoch 81/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1660 - ACC: 0.2111 - PPL: 175.6476\n",
      "Epoch 00081: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 870ms/step - loss: 5.1653 - ACC: 0.2111 - PPL: 175.5167 - val_loss: 5.1930 - val_ACC: 0.2167 - val_PPL: 180.4240\n",
      "Epoch 82/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1615 - ACC: 0.2117 - PPL: 174.7063\n",
      "Epoch 00082: val_PPL did not improve\n",
      "50/50 [==============================] - 43s 857ms/step - loss: 5.1617 - ACC: 0.2116 - PPL: 174.7436 - val_loss: 5.1752 - val_ACC: 0.2181 - val_PPL: 177.1993\n",
      "Epoch 83/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1480 - ACC: 0.2117 - PPL: 172.3953\n",
      "Epoch 00083: val_PPL improved from 174.83749 to 174.69161, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 871ms/step - loss: 5.1452 - ACC: 0.2118 - PPL: 171.9518 - val_loss: 5.1594 - val_ACC: 0.2215 - val_PPL: 174.6916\n",
      "Epoch 84/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1602 - ACC: 0.2121 - PPL: 174.6343\n",
      "Epoch 00084: val_PPL improved from 174.69161 to 172.52272, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 877ms/step - loss: 5.1600 - ACC: 0.2121 - PPL: 174.5956 - val_loss: 5.1475 - val_ACC: 0.2222 - val_PPL: 172.5227\n",
      "Epoch 85/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1394 - ACC: 0.2143 - PPL: 170.9948\n",
      "Epoch 00085: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 888ms/step - loss: 5.1398 - ACC: 0.2141 - PPL: 171.0487 - val_loss: 5.1654 - val_ACC: 0.2203 - val_PPL: 175.4580\n",
      "Epoch 86/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1406 - ACC: 0.2139 - PPL: 171.3114\n",
      "Epoch 00086: val_PPL did not improve\n",
      "50/50 [==============================] - 46s 925ms/step - loss: 5.1408 - ACC: 0.2139 - PPL: 171.3271 - val_loss: 5.1684 - val_ACC: 0.2197 - val_PPL: 176.0250\n",
      "Epoch 87/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1338 - ACC: 0.2143 - PPL: 170.0832\n",
      "Epoch 00087: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 889ms/step - loss: 5.1350 - ACC: 0.2141 - PPL: 170.2737 - val_loss: 5.1763 - val_ACC: 0.2151 - val_PPL: 177.2997\n",
      "Epoch 88/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1430 - ACC: 0.2135 - PPL: 171.7424\n",
      "Epoch 00088: val_PPL improved from 172.52272 to 171.42467, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 879ms/step - loss: 5.1423 - ACC: 0.2136 - PPL: 171.6071 - val_loss: 5.1418 - val_ACC: 0.2222 - val_PPL: 171.4247\n",
      "Epoch 89/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1327 - ACC: 0.2149 - PPL: 169.7122\n",
      "Epoch 00089: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 896ms/step - loss: 5.1304 - ACC: 0.2151 - PPL: 169.3422 - val_loss: 5.1585 - val_ACC: 0.2170 - val_PPL: 174.3544\n",
      "Epoch 90/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1361 - ACC: 0.2145 - PPL: 170.4979\n",
      "Epoch 00090: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 897ms/step - loss: 5.1365 - ACC: 0.2145 - PPL: 170.5559 - val_loss: 5.1692 - val_ACC: 0.2128 - val_PPL: 176.2530\n",
      "Epoch 91/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1052 - ACC: 0.2166 - PPL: 165.2903\n",
      "Epoch 00091: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 895ms/step - loss: 5.1051 - ACC: 0.2166 - PPL: 165.2632 - val_loss: 5.1832 - val_ACC: 0.2124 - val_PPL: 178.8861\n",
      "Epoch 92/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1150 - ACC: 0.2154 - PPL: 166.7657\n",
      "Epoch 00092: val_PPL improved from 171.42467 to 168.47321, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 45s 904ms/step - loss: 5.1158 - ACC: 0.2155 - PPL: 166.9013 - val_loss: 5.1241 - val_ACC: 0.2227 - val_PPL: 168.4732\n",
      "Epoch 93/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1128 - ACC: 0.2153 - PPL: 166.3788\n",
      "Epoch 00093: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 898ms/step - loss: 5.1116 - ACC: 0.2155 - PPL: 166.1844 - val_loss: 5.1462 - val_ACC: 0.2225 - val_PPL: 172.3018\n",
      "Epoch 94/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1122 - ACC: 0.2172 - PPL: 166.4933\n",
      "Epoch 00094: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 891ms/step - loss: 5.1121 - ACC: 0.2172 - PPL: 166.4672 - val_loss: 5.1329 - val_ACC: 0.2237 - val_PPL: 169.9630\n",
      "Epoch 95/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1103 - ACC: 0.2157 - PPL: 166.1186\n",
      "Epoch 00095: val_PPL did not improve\n",
      "50/50 [==============================] - 46s 920ms/step - loss: 5.1109 - ACC: 0.2158 - PPL: 166.2195 - val_loss: 5.1351 - val_ACC: 0.2224 - val_PPL: 170.2886\n",
      "Epoch 96/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0890 - ACC: 0.2192 - PPL: 162.5702\n",
      "Epoch 00096: val_PPL improved from 168.47321 to 166.70837, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 45s 896ms/step - loss: 5.0901 - ACC: 0.2191 - PPL: 162.7436 - val_loss: 5.1143 - val_ACC: 0.2241 - val_PPL: 166.7084\n",
      "Epoch 97/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0900 - ACC: 0.2184 - PPL: 162.8279\n",
      "Epoch 00097: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 878ms/step - loss: 5.0916 - ACC: 0.2182 - PPL: 163.0958 - val_loss: 5.1275 - val_ACC: 0.2229 - val_PPL: 168.9958\n",
      "Epoch 98/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0876 - ACC: 0.2183 - PPL: 162.3898\n",
      "Epoch 00098: val_PPL did not improve\n",
      "50/50 [==============================] - 45s 894ms/step - loss: 5.0877 - ACC: 0.2184 - PPL: 162.3910 - val_loss: 5.1166 - val_ACC: 0.2248 - val_PPL: 167.3093\n",
      "Epoch 99/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.1027 - ACC: 0.2187 - PPL: 164.8282\n",
      "Epoch 00099: val_PPL did not improve\n",
      "50/50 [==============================] - 44s 884ms/step - loss: 5.1023 - ACC: 0.2187 - PPL: 164.7503 - val_loss: 5.1258 - val_ACC: 0.2225 - val_PPL: 168.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0905 - ACC: 0.2180 - PPL: 162.7309\n",
      "Epoch 00100: val_PPL improved from 166.70837 to 166.46328, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 44s 886ms/step - loss: 5.0906 - ACC: 0.2180 - PPL: 162.7531 - val_loss: 5.1119 - val_ACC: 0.2246 - val_PPL: 166.4633\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(generator=gen_word_word(), \n",
    "                           steps_per_epoch=50, epochs=125,\n",
    "                           callbacks=callback_lists,\n",
    "                           validation_data=gen_word_word(dataset='valid'),\n",
    "                           validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2l21JdFPo3A"
   },
   "outputs": [],
   "source": [
    "history = pd.DataFrame(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "jQLqEZwRPo3M",
    "outputId": "9e0c5672-82b6-4fdb-8f11-739eb0007baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ACC', 'PPL', 'loss', 'val_ACC', 'val_PPL', 'val_loss'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1574705400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.columns)\n",
    "history.loc[1:,['PPL','val_PPL']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kI9DyqjoPo3s"
   },
   "outputs": [],
   "source": [
    "# path_model = local_path + 'model/model.keras'\n",
    "# model.save(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_USEcjEPo3x"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "model_restore = load_model(path_model, custom_objects={'ACC':ACC,'PPL': PPL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1129
    },
    "colab_type": "code",
    "id": "9iSl3szGPo37",
    "outputId": "150689c7-d3fb-4935-9184-dfc4ec6ef708",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.0887 - ACC: 0.2185 - PPL: 162.5365\n",
      "Epoch 00001: val_PPL did not improve\n",
      "50/50 [==============================] - 48s 959ms/step - loss: 5.0894 - ACC: 0.2186 - PPL: 162.6333 - val_loss: 5.1154 - val_ACC: 0.2261 - val_PPL: 166.7546\n"
     ]
    }
   ],
   "source": [
    "hist = model_restore.fit_generator(generator=gen_word_word(), \n",
    "                           steps_per_epoch=50, epochs=1,\n",
    "                           callbacks=callback_lists,\n",
    "                           validation_data=gen_word_word(dataset='valid'),\n",
    "                           validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "WPNTJojePo4D",
    "outputId": "8b2ce958-73e2-4498-9f01-78d2f5d49a86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ACC', 'PPL', 'loss', 'val_ACC', 'val_PPL', 'val_loss'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1574666b70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "print(history.columns)\n",
    "history.loc[:,['PPL','val_PPL']].tail(4600).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SY8V79vOPo4N"
   },
   "outputs": [],
   "source": [
    "def predict_seq(model, preSeq=None, genLen=seq_len, power=1):\n",
    "    \"\"\" Predict a sequence with length genLen.\n",
    "        arg:\n",
    "            model: Keras model used to predict.\n",
    "            preSeq: list. The leading sequence.\n",
    "            genLen: float or np.inf. If power is equal to np.inf, then an argmax will be used. \n",
    "            power: Probility power.\n",
    "    \"\"\"\n",
    "    preSeq = [word_to_id['<SS>']] if preSeq == None else [word_to_id['<SS>']] + preSeq   \n",
    "    pointer = len(preSeq) - 1\n",
    "    \n",
    "    for _ in range(genLen):\n",
    "        inputSeq = np.array([preSeq])\n",
    "        prob = model.predict(inputSeq)[0, pointer, :]\n",
    "        if power==np.inf:\n",
    "            pred = np.argmax(prob)\n",
    "        else:\n",
    "            prob = np.power(prob, power)\n",
    "            prob = prob / np.sum(prob)\n",
    "            pred = np.random.choice(range(voc_size), p=prob)\n",
    "        preSeq.append(pred)\n",
    "        pointer = pointer + 1\n",
    "\n",
    "    return preSeq, ' '.join([id_to_word[id] for id in preSeq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SS> mr. cable says <eos> according to refinancing notably millions profitable directors increased N to N N off the end of september <eos> stock-index arbitrage trading has led as an fall in the <unk> <EE> <EE>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, seq = predict_seq(model, power=1)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character aware model\n",
    "\n",
    "[Character-Aware Neural Language Models -- arxiv-1508.06615 -- AAAI 2016](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "[Ref: Github/jarfo/kchar](https://github.com/jarfo/kchar)\n",
    "\n",
    "\n",
    "![model](https://github.com/stikbuf/Language_Modeling/blob/master/Character%20aware.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKog3W-MPo4c"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'word length distribution, max=19, min=1')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_text = [id_to_word[idx] for idx in train_data]\n",
    "valid_data_text = [id_to_word[idx] for idx in valid_data]\n",
    "test_data_text = [id_to_word[idx] for idx in test_data]\n",
    "total_data_text = train_data_text + valid_data_text + test_data_text\n",
    "\n",
    "maxWordLen = max([len(word) for word in total_data_text])\n",
    "maxWordLen += 2 # Start and End character\n",
    "\n",
    "ds = pd.Series([len(word) for word in total_data_text])\n",
    "plt.figure(figsize=(10,5))\n",
    "ds.plot.hist(bins=range(1, maxWordLen))\n",
    "plt.title('word length distribution, max={0}, min={1}'.\n",
    "          format(ds.max(), ds.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars: 51 \n",
      "\n",
      "['1', 'a', 'j', 'u', '6', '3', 'E', '-', '$', '/', 'c', 'q', 'z', '2', 'x', 'f', '4', 'm', 't', 'n', 'l', '7', '5', 'd', '8', '.', '0', 'v', '9', 'k', '<', 'i', 'w', '&', 'o', 'S', ' ', 'b', 's', 'p', 'y', 'r', 'N', \"'\", '*', 'e', 'h', '#', 'g', '\\\\', '>'] \n",
      "\n",
      "{0: '1', 1: 'a', 2: 'j', 3: 'u', 4: '6', 5: '3', 6: 'E', 7: '-', 8: '$', 9: '/', 10: 'c', 11: 'q', 12: 'z', 13: '2', 14: 'x', 15: 'f', 16: '4', 17: 'm', 18: 't', 19: 'n', 20: 'l', 21: '7', 22: '5', 23: 'd', 24: '8', 25: '.', 26: '0', 27: 'v', 28: '9', 29: 'k', 30: '<', 31: 'i', 32: 'w', 33: '&', 34: 'o', 35: 'S', 36: ' ', 37: 'b', 38: 's', 39: 'p', 40: 'y', 41: 'r', 42: 'N', 43: \"'\", 44: '*', 45: 'e', 46: 'h', 47: '#', 48: 'g', 49: '\\\\', 50: '>'} \n",
      "\n",
      "{'#': 47, '1': 0, 'j': 2, 'u': 3, '3': 5, 'E': 6, '-': 7, '$': 8, '/': 9, 'q': 11, 'z': 12, \"'\": 43, 'p': 39, '2': 13, 'c': 10, '4': 16, 'f': 15, 'm': 17, 't': 18, 'a': 1, 'n': 19, 'l': 20, '7': 21, 'y': 40, '5': 22, '8': 24, '.': 25, '0': 26, '9': 28, 'k': 29, '<': 30, 'i': 31, 'w': 32, '&': 33, 'S': 35, ' ': 36, 'd': 23, 'b': 37, 's': 38, '6': 4, 'r': 41, 'N': 42, '*': 44, 'e': 45, 'h': 46, 'x': 14, 'g': 48, '\\\\': 49, 'v': 27, 'o': 34, '>': 50} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chars = []\n",
    "for word in total_data_text:\n",
    "    chars.extend(list(word))\n",
    "    \n",
    "ds = pd.Series(chars)\n",
    "plt.figure(figsize=(15,10))   \n",
    "matplotlib.rc('xtick', labelsize=20)\n",
    "matplotlib.rc('ytick', labelsize=15)\n",
    "ds.value_counts().plot.bar()\n",
    "plt.title('character distribution, total {0} characters(without \\'S\\' and \\'E\\')'.format(len(set(chars))))\n",
    "plt.show()\n",
    "\n",
    "chars = list(set(chars + ['S'] + ['E'] + [' ']))\n",
    "# 'S' for word leading char, 'E' for word ending char, space for padding\n",
    "id_to_chars = dict(enumerate(chars))\n",
    "chars_to_id = dict((v, k) for k,v in id_to_chars.items())\n",
    "num_chars = len(chars)\n",
    "\n",
    "print('number of chars:', num_chars, '\\n')\n",
    "print(chars, '\\n')\n",
    "print(id_to_chars, '\\n')\n",
    "print(chars_to_id, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 18,\n",
       " 46,\n",
       " 45,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 6]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_to_charId(wordId):\n",
    "    # Convert word to a string of word Ids\n",
    "    wordString = 'S' + id_to_word[wordId].center(maxWordLen - 2) + 'E'\n",
    "    return [chars_to_id[char] for char in wordString]\n",
    "\n",
    "def wordSeq_charSeq(bWordSeq):\n",
    "    batch, seqLen = bWordSeq.shape\n",
    "    bWordSeq = bWordSeq.ravel()\n",
    "    charSeq = np.array([word_to_charId(wordId) for wordId in bWordSeq])\n",
    "    return charSeq.reshape(batch, seqLen, -1)\n",
    "\n",
    "word_to_charId(word_to_id['the']) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical \n",
    "\n",
    "def gen_char_word(batch_size=128, dataset='train'):\n",
    "    assert dataset in ['train', 'valid', 'test'], 'Dataset must be train or valid or test.'\n",
    "    \n",
    "    dic = {'train':train_data, 'valid':valid_data, 'test':test_data}\n",
    "    data = dic[dataset]\n",
    "    \n",
    "    while True:\n",
    "        rnd_idxs = list(range(len(data)-seq_len-1))\n",
    "        random.shuffle(rnd_idxs)\n",
    "        cnt = 0\n",
    "        while cnt < len(rnd_idxs) - batch_size :\n",
    "            X = np.array([[word_to_id['<SS>']] + data[i:i+seq_len] + [word_to_id['<EE>']]\n",
    "                          for i in rnd_idxs[cnt:cnt+batch_size]])\n",
    "            Y = X[:,1:]\n",
    "            X = X[:,:-1]\n",
    "            Y = to_categorical(Y)\n",
    "            X = wordSeq_charSeq(X) \n",
    "            #print(X.shape)\n",
    "            cnt += batch_size\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10002,)\n"
     ]
    }
   ],
   "source": [
    "print(next(gen_char_word(batch_size=1, dataset='train'))[1][0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 20\n",
    "        self.seq_length = seq_len + 1\n",
    "        self.max_word_l = maxWordLen\n",
    "        self.char_vocab_size = num_chars\n",
    "        self.char_vec_size = 15\n",
    "        self.feature_maps = [50,100,150,200,200,200,200]\n",
    "        self.kernels = [1,2,3,4,5,6,7]\n",
    "        self.highway_layers = 2\n",
    "        self.num_lstm_layers = 2\n",
    "        self.rnn_size = 128\n",
    "        self.word_vocab_size = voc_size\n",
    "        self.dropout = 0.5\n",
    "        self.learing_rate = 1e-5\n",
    "        \n",
    "opt = Option()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "![CNN part](https://github.com/stikbuf/Language_Modeling/blob/master/Character%20aware-CNN.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Concatenate, Reshape\n",
    "\n",
    "def CNN(seq_length, length, feature_maps, kernels, x):\n",
    "\n",
    "    concat_input = []\n",
    "    for feature_map, kernel in zip(feature_maps, kernels):\n",
    "        reduced_l = length - kernel + 1\n",
    "        conv = Conv2D(feature_map, (1, kernel), activation='tanh', data_format=\"channels_last\")(x)\n",
    "        maxp = MaxPooling2D((1, reduced_l), data_format=\"channels_last\")(conv)\n",
    "        concat_input.append(maxp)\n",
    "\n",
    "    x = Concatenate()(concat_input)\n",
    "    x = Reshape((seq_length, sum(feature_maps)))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Network  \n",
    "[Srivastava et al.](https://arxiv.org/abs/1505.00387)\n",
    "\n",
    "Input vector is $\\textbf{y}$, then layer output $\\textbf{z}$ is\n",
    "$$\\textbf{z = t} \\odot g(\\textbf{W}_H\\textbf{y}+\\textbf{b}_H) + \\textbf{(1 - t)} \\odot \\textbf{y}$$\n",
    "where \n",
    "$$\\textbf{t} = \\sigma(\\textbf{W}_T\\textbf{y}+\\textbf{b}_T)$$\n",
    "\n",
    "$\\textbf{t}$ is called the\n",
    "*transform gate*, and $(\\textbf{1}\\textbf{t})$ is called the *carry gate*. \n",
    "Similar to the memory cells in LSTM networks, highway layers allow for training of deep networks by adaptively carrying some dimensions of the input directly to the output. By construction the dimensions of $\\textbf{y}$ and $\\textbf{z}$ have to match, and hence $\\textbf{W}_T$ and $\\textbf{W}_H$ are square matrices.  \n",
    "\n",
    "A keras model is also a keras layer! So you can combine some keras layers to design your own layer. This is useful when combining with TimeDistributed wrapper. [See section 3 in this blog](https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Multiply, Add, Lambda, Input\n",
    "from tensorflow.python.keras.initializers import Constant\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "def Highway(value, nLayers, activation='tanh', gateBias=-3):\n",
    "    dim = K.int_shape(value)[-1]\n",
    "    gateBiasInitalizer = Constant(gateBias)\n",
    "    for i in range(nLayers):\n",
    "        tGate = Dense(units=dim, bias_initializer=gateBiasInitalizer)(value)\n",
    "        tGate = Activation('sigmoid')(tGate)\n",
    "        cGate = Lambda(lambda x: 1.0-x)(tGate) # I do not specify output_shape\n",
    "        transformed = Dense(units=dim, bias_initializer=gateBiasInitalizer)(value)\n",
    "        transformed = Activation(activation)(value)\n",
    "        transformedGate = Multiply()([tGate, transformed])\n",
    "        identityGate = Multiply()([cGate, value])\n",
    "        value = Add()([transformedGate, identityGate])\n",
    "    return value\n",
    "\n",
    "inputs = Input((sum(opt.feature_maps),))\n",
    "HighwayLayer = Model(inputs=inputs, outputs=Highway(inputs, nLayers=opt.highway_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "chars (InputLayer)              (20, 36, 21)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "chars_embedding (Embedding)     (20, 36, 21, 15)     765         chars[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (20, 36, 21, 50)     800         chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (20, 36, 20, 100)    3100        chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (20, 36, 19, 150)    6900        chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (20, 36, 18, 200)    12200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (20, 36, 17, 200)    15200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (20, 36, 16, 200)    18200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (20, 36, 15, 200)    21200       chars_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (20, 36, 1, 50)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (20, 36, 1, 100)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (20, 36, 1, 150)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (20, 36, 1, 200)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (20, 36, 1, 200)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (20, 36, 1, 200)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (20, 36, 1, 200)     0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (20, 36, 1, 1100)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (20, 36, 1100)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (20, 36, 1100)       4400        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (20, 36, 1100)       2422200     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (20, 36, 128)        629248      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (20, 36, 128)        0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (20, 36, 128)        131584      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (20, 36, 128)        0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (20, 36, 10002)      1290258     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,556,055\n",
      "Trainable params: 4,553,855\n",
      "Non-trainable params: 2,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization, TimeDistributed\n",
    "#from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "chars = Input(batch_shape=(opt.batch_size, opt.seq_length, opt.max_word_l), name='chars')\n",
    "chars_embedding = Embedding(opt.char_vocab_size, opt.char_vec_size, name='chars_embedding')(chars)\n",
    "cnn = CNN(opt.seq_length, opt.max_word_l, opt.feature_maps, opt.kernels, chars_embedding)\n",
    "x = cnn\n",
    "inputs = chars\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = TimeDistributed(HighwayLayer)(x)\n",
    "\n",
    "for l in range(opt.num_lstm_layers):\n",
    "    x = LSTM(opt.rnn_size, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, stateful=True)(x)\n",
    "\n",
    "    if opt.dropout > 0:\n",
    "        x = Dropout(opt.dropout)(x)\n",
    "        \n",
    "output = TimeDistributed(Dense(opt.word_vocab_size, activation='softmax'))(x)\n",
    "\n",
    "modelCAware = Model(inputs=inputs, outputs=output)\n",
    "modelCAware.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity\n",
    "def PPL(y_true, y_pred):\n",
    "    return tf.exp(tf.reduce_mean(tf.keras.backend.categorical_crossentropy(y_true, y_pred)))\n",
    "\n",
    "def ACC(y_true, y_pred):\n",
    "    ACC = tf.equal(tf.argmax(y_true, axis = 2), \n",
    "                   tf.argmax(y_pred, axis = 2))\n",
    "    ACC = tf.cast(ACC, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "\n",
    "optimizer = RMSprop(lr=opt.learing_rate)\n",
    "modelCAware.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[ACC, PPL])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import os\n",
    "if not os.path.exists(local_path + 'model/'):\n",
    "    os.mkdir(local_path + 'model/')\n",
    "\n",
    "path_model = local_path + 'model/model.keras'    \n",
    "tensorboard = TensorBoard(log_dir='log')\n",
    "checkpoint = ModelCheckpoint(filepath=path_model, verbose=1,\n",
    "                             monitor='val_PPL',mode='min' ,save_best_only='True')\n",
    "# path_model = local_path + 'model/model.keras'\n",
    "# model.save(path_model)\n",
    "\n",
    "callback_lists=[tensorboard,checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 7.0308 - ACC: 0.0471 - PPL: 1744.3669\n",
      "Epoch 00001: val_PPL improved from inf to 23562.97281, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 26s 518ms/step - loss: 7.0257 - ACC: 0.0474 - PPL: 1726.9807 - val_loss: 10.0141 - val_ACC: 0.0015 - val_PPL: 23562.9728\n",
      "Epoch 2/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.5397 - ACC: 0.0593 - PPL: 695.9500\n",
      "Epoch 00002: val_PPL improved from 23562.97281 to 728.44933, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 346ms/step - loss: 6.5392 - ACC: 0.0597 - PPL: 695.5208 - val_loss: 6.5851 - val_ACC: 0.0520 - val_PPL: 728.4493\n",
      "Epoch 3/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.4736 - ACC: 0.0739 - PPL: 653.9884\n",
      "Epoch 00003: val_PPL improved from 728.44933 to 678.89578, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 6.4715 - ACC: 0.0744 - PPL: 652.6250 - val_loss: 6.5134 - val_ACC: 0.0794 - val_PPL: 678.8958\n",
      "Epoch 4/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.4565 - ACC: 0.0905 - PPL: 641.6471\n",
      "Epoch 00004: val_PPL improved from 678.89578 to 607.90120, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 6.4552 - ACC: 0.0904 - PPL: 640.7700 - val_loss: 6.4000 - val_ACC: 0.0906 - val_PPL: 607.9012\n",
      "Epoch 5/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3755 - ACC: 0.0985 - PPL: 592.0830\n",
      "Epoch 00005: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 6.3731 - ACC: 0.0986 - PPL: 590.6595 - val_loss: 6.4936 - val_ACC: 0.0742 - val_PPL: 666.2581\n",
      "Epoch 6/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3794 - ACC: 0.1032 - PPL: 593.9234\n",
      "Epoch 00006: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 6.3791 - ACC: 0.1032 - PPL: 593.6453 - val_loss: 6.4823 - val_ACC: 0.0744 - val_PPL: 660.7695\n",
      "Epoch 7/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3228 - ACC: 0.1108 - PPL: 563.7302\n",
      "Epoch 00007: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 6.3225 - ACC: 0.1106 - PPL: 563.4139 - val_loss: 6.4845 - val_ACC: 0.0717 - val_PPL: 661.6813\n",
      "Epoch 8/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.3272 - ACC: 0.1086 - PPL: 562.9195\n",
      "Epoch 00008: val_PPL improved from 607.90120 to 539.19171, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 6.3265 - ACC: 0.1087 - PPL: 562.4763 - val_loss: 6.2816 - val_ACC: 0.1096 - val_PPL: 539.1917\n",
      "Epoch 9/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.2234 - ACC: 0.1174 - PPL: 510.2618\n",
      "Epoch 00009: val_PPL improved from 539.19171 to 505.02588, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 6.2234 - ACC: 0.1171 - PPL: 510.1317 - val_loss: 6.2164 - val_ACC: 0.1120 - val_PPL: 505.0259\n",
      "Epoch 10/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.2503 - ACC: 0.1170 - PPL: 524.7958\n",
      "Epoch 00010: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 6.2541 - ACC: 0.1168 - PPL: 526.8753 - val_loss: 6.2395 - val_ACC: 0.1097 - val_PPL: 517.7707\n",
      "Epoch 11/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.2037 - ACC: 0.1204 - PPL: 501.7947\n",
      "Epoch 00011: val_PPL improved from 505.02588 to 494.05822, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 6.2029 - ACC: 0.1203 - PPL: 501.2308 - val_loss: 6.1948 - val_ACC: 0.1159 - val_PPL: 494.0582\n",
      "Epoch 12/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.2210 - ACC: 0.1178 - PPL: 508.9812\n",
      "Epoch 00012: val_PPL improved from 494.05822 to 466.19062, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 346ms/step - loss: 6.2176 - ACC: 0.1183 - PPL: 507.2902 - val_loss: 6.1355 - val_ACC: 0.1211 - val_PPL: 466.1906\n",
      "Epoch 13/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1406 - ACC: 0.1243 - PPL: 470.0259\n",
      "Epoch 00013: val_PPL improved from 466.19062 to 465.20905, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 6.1377 - ACC: 0.1244 - PPL: 468.6688 - val_loss: 6.1260 - val_ACC: 0.1240 - val_PPL: 465.2090\n",
      "Epoch 14/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1358 - ACC: 0.1250 - PPL: 466.3676\n",
      "Epoch 00014: val_PPL improved from 465.20905 to 428.99792, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 6.1350 - ACC: 0.1249 - PPL: 465.9479 - val_loss: 6.0500 - val_ACC: 0.1269 - val_PPL: 428.9979\n",
      "Epoch 15/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1498 - ACC: 0.1263 - PPL: 473.3292\n",
      "Epoch 00015: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 6.1488 - ACC: 0.1259 - PPL: 472.7788 - val_loss: 6.0868 - val_ACC: 0.1236 - val_PPL: 444.4685\n",
      "Epoch 16/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1288 - ACC: 0.1289 - PPL: 463.6478\n",
      "Epoch 00016: val_PPL improved from 428.99792 to 421.90048, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 6.1232 - ACC: 0.1288 - PPL: 461.3046 - val_loss: 6.0273 - val_ACC: 0.1325 - val_PPL: 421.9005\n",
      "Epoch 17/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.1125 - ACC: 0.1268 - PPL: 455.3606\n",
      "Epoch 00017: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 6.1169 - ACC: 0.1264 - PPL: 457.4612 - val_loss: 6.0639 - val_ACC: 0.1302 - val_PPL: 434.0421\n",
      "Epoch 18/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0797 - ACC: 0.1311 - PPL: 441.6923\n",
      "Epoch 00018: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 6.0800 - ACC: 0.1311 - PPL: 441.7501 - val_loss: 6.0444 - val_ACC: 0.1276 - val_PPL: 425.2405\n",
      "Epoch 19/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0598 - ACC: 0.1320 - PPL: 431.8100\n",
      "Epoch 00019: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 6.0532 - ACC: 0.1324 - PPL: 429.3173 - val_loss: 6.0440 - val_ACC: 0.1323 - val_PPL: 424.6163\n",
      "Epoch 20/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0568 - ACC: 0.1335 - PPL: 430.3662\n",
      "Epoch 00020: val_PPL improved from 421.90048 to 384.90851, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 346ms/step - loss: 6.0572 - ACC: 0.1336 - PPL: 430.4855 - val_loss: 5.9417 - val_ACC: 0.1397 - val_PPL: 384.9085\n",
      "Epoch 21/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0572 - ACC: 0.1292 - PPL: 432.5331\n",
      "Epoch 00021: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 6.0538 - ACC: 0.1295 - PPL: 431.0859 - val_loss: 6.0171 - val_ACC: 0.1300 - val_PPL: 415.1088\n",
      "Epoch 22/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0291 - ACC: 0.1348 - PPL: 419.3906\n",
      "Epoch 00022: val_PPL improved from 384.90851 to 377.59266, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 6.0271 - ACC: 0.1348 - PPL: 418.5337 - val_loss: 5.9187 - val_ACC: 0.1455 - val_PPL: 377.5927\n",
      "Epoch 23/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0150 - ACC: 0.1383 - PPL: 413.3086\n",
      "Epoch 00023: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 6.0148 - ACC: 0.1382 - PPL: 413.1321 - val_loss: 5.9745 - val_ACC: 0.1438 - val_PPL: 396.2204\n",
      "Epoch 24/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 6.0180 - ACC: 0.1388 - PPL: 416.2550\n",
      "Epoch 00024: val_PPL improved from 377.59266 to 370.84301, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 345ms/step - loss: 6.0203 - ACC: 0.1385 - PPL: 417.1195 - val_loss: 5.9042 - val_ACC: 0.1421 - val_PPL: 370.8430\n",
      "Epoch 25/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9752 - ACC: 0.1434 - PPL: 397.1479\n",
      "Epoch 00025: val_PPL improved from 370.84301 to 347.30688, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.9747 - ACC: 0.1435 - PPL: 396.8822 - val_loss: 5.8309 - val_ACC: 0.1570 - val_PPL: 347.3069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9877 - ACC: 0.1449 - PPL: 403.8615\n",
      "Epoch 00026: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.9853 - ACC: 0.1450 - PPL: 402.8510 - val_loss: 5.9804 - val_ACC: 0.1375 - val_PPL: 400.9195\n",
      "Epoch 27/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9931 - ACC: 0.1427 - PPL: 406.0522\n",
      "Epoch 00027: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.9972 - ACC: 0.1429 - PPL: 407.7861 - val_loss: 5.8858 - val_ACC: 0.1426 - val_PPL: 362.4547\n",
      "Epoch 28/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9858 - ACC: 0.1465 - PPL: 401.1374\n",
      "Epoch 00028: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.9806 - ACC: 0.1469 - PPL: 399.2463 - val_loss: 5.8960 - val_ACC: 0.1502 - val_PPL: 366.6716\n",
      "Epoch 29/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9539 - ACC: 0.1477 - PPL: 391.1446\n",
      "Epoch 00029: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 5.9566 - ACC: 0.1475 - PPL: 392.1435 - val_loss: 5.9353 - val_ACC: 0.1467 - val_PPL: 382.3411\n",
      "Epoch 30/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9887 - ACC: 0.1447 - PPL: 404.7339\n",
      "Epoch 00030: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 5.9874 - ACC: 0.1446 - PPL: 404.1210 - val_loss: 5.9414 - val_ACC: 0.1402 - val_PPL: 387.2739\n",
      "Epoch 31/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9607 - ACC: 0.1470 - PPL: 391.4494\n",
      "Epoch 00031: val_PPL improved from 347.30688 to 341.48045, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 337ms/step - loss: 5.9650 - ACC: 0.1468 - PPL: 393.2259 - val_loss: 5.8205 - val_ACC: 0.1494 - val_PPL: 341.4804\n",
      "Epoch 32/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9497 - ACC: 0.1462 - PPL: 387.9827\n",
      "Epoch 00032: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.9451 - ACC: 0.1462 - PPL: 386.3097 - val_loss: 5.9753 - val_ACC: 0.1436 - val_PPL: 398.1499\n",
      "Epoch 33/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9427 - ACC: 0.1492 - PPL: 385.1159\n",
      "Epoch 00033: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.9461 - ACC: 0.1491 - PPL: 386.4199 - val_loss: 5.8956 - val_ACC: 0.1481 - val_PPL: 366.4621\n",
      "Epoch 34/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9299 - ACC: 0.1529 - PPL: 381.3048\n",
      "Epoch 00034: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 332ms/step - loss: 5.9283 - ACC: 0.1533 - PPL: 380.6438 - val_loss: 5.9151 - val_ACC: 0.1367 - val_PPL: 373.2311\n",
      "Epoch 35/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9072 - ACC: 0.1571 - PPL: 371.3021\n",
      "Epoch 00035: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 337ms/step - loss: 5.9085 - ACC: 0.1574 - PPL: 371.7302 - val_loss: 5.8271 - val_ACC: 0.1497 - val_PPL: 345.1921\n",
      "Epoch 36/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9369 - ACC: 0.1501 - PPL: 382.6123\n",
      "Epoch 00036: val_PPL improved from 341.48045 to 335.06956, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.9343 - ACC: 0.1505 - PPL: 381.6182 - val_loss: 5.7990 - val_ACC: 0.1581 - val_PPL: 335.0696\n",
      "Epoch 37/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8981 - ACC: 0.1564 - PPL: 368.8846\n",
      "Epoch 00037: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 5.9008 - ACC: 0.1567 - PPL: 369.8389 - val_loss: 5.8509 - val_ACC: 0.1553 - val_PPL: 351.6547\n",
      "Epoch 38/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9081 - ACC: 0.1546 - PPL: 372.7141\n",
      "Epoch 00038: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 5.9127 - ACC: 0.1546 - PPL: 374.5000 - val_loss: 5.9267 - val_ACC: 0.1526 - val_PPL: 378.8890\n",
      "Epoch 39/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8862 - ACC: 0.1559 - PPL: 365.0319\n",
      "Epoch 00039: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8821 - ACC: 0.1561 - PPL: 363.6063 - val_loss: 5.8467 - val_ACC: 0.1519 - val_PPL: 349.9238\n",
      "Epoch 40/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9074 - ACC: 0.1556 - PPL: 371.8650\n",
      "Epoch 00040: val_PPL improved from 335.06956 to 324.91857, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 345ms/step - loss: 5.9090 - ACC: 0.1556 - PPL: 372.3682 - val_loss: 5.7740 - val_ACC: 0.1688 - val_PPL: 324.9186\n",
      "Epoch 41/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8593 - ACC: 0.1587 - PPL: 354.2866\n",
      "Epoch 00041: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8631 - ACC: 0.1585 - PPL: 355.7034 - val_loss: 5.8148 - val_ACC: 0.1617 - val_PPL: 339.9570\n",
      "Epoch 42/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8804 - ACC: 0.1612 - PPL: 362.2739\n",
      "Epoch 00042: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8840 - ACC: 0.1611 - PPL: 363.5953 - val_loss: 5.8033 - val_ACC: 0.1563 - val_PPL: 337.2097\n",
      "Epoch 43/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9034 - ACC: 0.1564 - PPL: 370.0689\n",
      "Epoch 00043: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 5.9071 - ACC: 0.1563 - PPL: 371.5079 - val_loss: 5.7866 - val_ACC: 0.1631 - val_PPL: 330.3309\n",
      "Epoch 44/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.9097 - ACC: 0.1595 - PPL: 372.7098\n",
      "Epoch 00044: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 5.9096 - ACC: 0.1596 - PPL: 372.6121 - val_loss: 5.7859 - val_ACC: 0.1579 - val_PPL: 329.6760\n",
      "Epoch 45/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8401 - ACC: 0.1601 - PPL: 346.7719\n",
      "Epoch 00045: val_PPL improved from 324.91857 to 321.13189, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.8344 - ACC: 0.1603 - PPL: 344.9988 - val_loss: 5.7574 - val_ACC: 0.1683 - val_PPL: 321.1319\n",
      "Epoch 46/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8593 - ACC: 0.1649 - PPL: 354.8568\n",
      "Epoch 00046: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.8566 - ACC: 0.1648 - PPL: 353.8993 - val_loss: 5.8178 - val_ACC: 0.1593 - val_PPL: 339.5286\n",
      "Epoch 47/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8570 - ACC: 0.1633 - PPL: 354.1031\n",
      "Epoch 00047: val_PPL improved from 321.13189 to 312.04488, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 345ms/step - loss: 5.8553 - ACC: 0.1634 - PPL: 353.4491 - val_loss: 5.7301 - val_ACC: 0.1723 - val_PPL: 312.0449\n",
      "Epoch 48/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8419 - ACC: 0.1644 - PPL: 350.0662\n",
      "Epoch 00048: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 5.8408 - ACC: 0.1646 - PPL: 349.5840 - val_loss: 5.7707 - val_ACC: 0.1631 - val_PPL: 324.4946\n",
      "Epoch 49/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8524 - ACC: 0.1613 - PPL: 351.8760\n",
      "Epoch 00049: val_PPL improved from 312.04488 to 306.96141, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 5.8575 - ACC: 0.1610 - PPL: 353.8164 - val_loss: 5.7165 - val_ACC: 0.1656 - val_PPL: 306.9614\n",
      "Epoch 50/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8579 - ACC: 0.1621 - PPL: 354.0628\n",
      "Epoch 00050: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8550 - ACC: 0.1625 - PPL: 353.0489 - val_loss: 6.0851 - val_ACC: 0.1316 - val_PPL: 443.7106\n",
      "Epoch 51/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8114 - ACC: 0.1691 - PPL: 337.8462\n",
      "Epoch 00051: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.8144 - ACC: 0.1685 - PPL: 338.8503 - val_loss: 5.7368 - val_ACC: 0.1690 - val_PPL: 314.9551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8289 - ACC: 0.1681 - PPL: 343.5215\n",
      "Epoch 00052: val_PPL improved from 306.96141 to 296.66874, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 349ms/step - loss: 5.8293 - ACC: 0.1679 - PPL: 343.5911 - val_loss: 5.6758 - val_ACC: 0.1756 - val_PPL: 296.6687\n",
      "Epoch 53/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8803 - ACC: 0.1652 - PPL: 361.6780\n",
      "Epoch 00053: val_PPL improved from 296.66874 to 290.46956, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.8788 - ACC: 0.1655 - PPL: 361.0799 - val_loss: 5.6635 - val_ACC: 0.1772 - val_PPL: 290.4696\n",
      "Epoch 54/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8147 - ACC: 0.1691 - PPL: 338.2400\n",
      "Epoch 00054: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8163 - ACC: 0.1691 - PPL: 338.7172 - val_loss: 5.7245 - val_ACC: 0.1722 - val_PPL: 308.9892\n",
      "Epoch 55/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8329 - ACC: 0.1666 - PPL: 345.2447\n",
      "Epoch 00055: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.8353 - ACC: 0.1666 - PPL: 346.0332 - val_loss: 5.7040 - val_ACC: 0.1722 - val_PPL: 302.8966\n",
      "Epoch 56/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8072 - ACC: 0.1678 - PPL: 337.9639\n",
      "Epoch 00056: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 5.8103 - ACC: 0.1678 - PPL: 338.9818 - val_loss: 5.7602 - val_ACC: 0.1625 - val_PPL: 321.3362\n",
      "Epoch 57/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7929 - ACC: 0.1703 - PPL: 331.1321\n",
      "Epoch 00057: val_PPL improved from 290.46956 to 279.34392, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.7958 - ACC: 0.1698 - PPL: 332.1080 - val_loss: 5.6186 - val_ACC: 0.1866 - val_PPL: 279.3439\n",
      "Epoch 58/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8385 - ACC: 0.1657 - PPL: 346.9950\n",
      "Epoch 00058: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.8393 - ACC: 0.1655 - PPL: 347.2098 - val_loss: 5.7206 - val_ACC: 0.1713 - val_PPL: 309.0430\n",
      "Epoch 59/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8221 - ACC: 0.1673 - PPL: 342.4643\n",
      "Epoch 00059: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 339ms/step - loss: 5.8252 - ACC: 0.1669 - PPL: 343.5135 - val_loss: 5.6844 - val_ACC: 0.1786 - val_PPL: 296.8952\n",
      "Epoch 60/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8179 - ACC: 0.1706 - PPL: 339.9051\n",
      "Epoch 00060: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.8164 - ACC: 0.1706 - PPL: 339.3592 - val_loss: 5.7801 - val_ACC: 0.1690 - val_PPL: 327.0333\n",
      "Epoch 61/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7822 - ACC: 0.1728 - PPL: 329.8902\n",
      "Epoch 00061: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.7843 - ACC: 0.1723 - PPL: 330.5157 - val_loss: 5.6272 - val_ACC: 0.1850 - val_PPL: 282.2214\n",
      "Epoch 62/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8214 - ACC: 0.1743 - PPL: 341.2982\n",
      "Epoch 00062: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 5.8229 - ACC: 0.1740 - PPL: 341.7546 - val_loss: 5.6263 - val_ACC: 0.1816 - val_PPL: 283.2174\n",
      "Epoch 63/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8373 - ACC: 0.1714 - PPL: 348.3774\n",
      "Epoch 00063: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 5.8386 - ACC: 0.1716 - PPL: 348.7360 - val_loss: 5.6706 - val_ACC: 0.1779 - val_PPL: 293.7170\n",
      "Epoch 64/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8157 - ACC: 0.1724 - PPL: 341.2613\n",
      "Epoch 00064: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8190 - ACC: 0.1723 - PPL: 342.3536 - val_loss: 5.7002 - val_ACC: 0.1716 - val_PPL: 302.6445\n",
      "Epoch 65/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8372 - ACC: 0.1683 - PPL: 347.6194\n",
      "Epoch 00065: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.8340 - ACC: 0.1691 - PPL: 346.5243 - val_loss: 5.6776 - val_ACC: 0.1789 - val_PPL: 295.9938\n",
      "Epoch 66/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7854 - ACC: 0.1759 - PPL: 329.1667\n",
      "Epoch 00066: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 5.7860 - ACC: 0.1758 - PPL: 329.3018 - val_loss: 5.6276 - val_ACC: 0.1828 - val_PPL: 280.8174\n",
      "Epoch 67/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7721 - ACC: 0.1764 - PPL: 325.5498\n",
      "Epoch 00067: val_PPL improved from 279.34392 to 278.81922, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 344ms/step - loss: 5.7728 - ACC: 0.1762 - PPL: 325.7182 - val_loss: 5.6142 - val_ACC: 0.1869 - val_PPL: 278.8192\n",
      "Epoch 68/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7792 - ACC: 0.1748 - PPL: 327.0138\n",
      "Epoch 00068: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.7794 - ACC: 0.1749 - PPL: 327.0026 - val_loss: 5.6480 - val_ACC: 0.1828 - val_PPL: 287.4873\n",
      "Epoch 69/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7915 - ACC: 0.1733 - PPL: 331.7940\n",
      "Epoch 00069: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.7884 - ACC: 0.1737 - PPL: 330.7518 - val_loss: 5.6533 - val_ACC: 0.1812 - val_PPL: 290.6185\n",
      "Epoch 70/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7769 - ACC: 0.1756 - PPL: 325.8026\n",
      "Epoch 00070: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 5.7779 - ACC: 0.1754 - PPL: 326.0738 - val_loss: 5.6366 - val_ACC: 0.1817 - val_PPL: 284.9774\n",
      "Epoch 71/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.8118 - ACC: 0.1728 - PPL: 337.0800\n",
      "Epoch 00071: val_PPL improved from 278.81922 to 268.46465, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.8095 - ACC: 0.1729 - PPL: 336.3011 - val_loss: 5.5773 - val_ACC: 0.1914 - val_PPL: 268.4647\n",
      "Epoch 72/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7998 - ACC: 0.1737 - PPL: 334.6173\n",
      "Epoch 00072: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.7967 - ACC: 0.1737 - PPL: 333.5761 - val_loss: 5.6608 - val_ACC: 0.1796 - val_PPL: 289.6528\n",
      "Epoch 73/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7851 - ACC: 0.1748 - PPL: 329.5397\n",
      "Epoch 00073: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.7827 - ACC: 0.1751 - PPL: 328.7237 - val_loss: 5.7110 - val_ACC: 0.1723 - val_PPL: 304.3775\n",
      "Epoch 74/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7642 - ACC: 0.1755 - PPL: 321.3982\n",
      "Epoch 00074: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.7629 - ACC: 0.1759 - PPL: 320.9197 - val_loss: 5.7901 - val_ACC: 0.1561 - val_PPL: 330.9336\n",
      "Epoch 75/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7541 - ACC: 0.1767 - PPL: 319.7733\n",
      "Epoch 00075: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 341ms/step - loss: 5.7537 - ACC: 0.1764 - PPL: 319.5450 - val_loss: 5.7177 - val_ACC: 0.1681 - val_PPL: 308.7041\n",
      "Epoch 76/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7387 - ACC: 0.1811 - PPL: 315.9096\n",
      "Epoch 00076: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 340ms/step - loss: 5.7441 - ACC: 0.1807 - PPL: 317.7046 - val_loss: 5.6075 - val_ACC: 0.1796 - val_PPL: 276.1849\n",
      "Epoch 77/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7850 - ACC: 0.1768 - PPL: 328.6289\n",
      "Epoch 00077: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 337ms/step - loss: 5.7825 - ACC: 0.1772 - PPL: 327.8127 - val_loss: 5.6352 - val_ACC: 0.1818 - val_PPL: 283.5444\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/50 [============================>.] - ETA: 0s - loss: 5.7929 - ACC: 0.1785 - PPL: 332.8624\n",
      "Epoch 00078: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 327ms/step - loss: 5.7933 - ACC: 0.1786 - PPL: 332.8898 - val_loss: 5.6505 - val_ACC: 0.1842 - val_PPL: 287.5910\n",
      "Epoch 79/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7868 - ACC: 0.1761 - PPL: 330.1013\n",
      "Epoch 00079: val_PPL improved from 268.46465 to 267.34196, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 5.7834 - ACC: 0.1763 - PPL: 328.9923 - val_loss: 5.5777 - val_ACC: 0.1901 - val_PPL: 267.3420\n",
      "Epoch 80/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7826 - ACC: 0.1720 - PPL: 327.8708\n",
      "Epoch 00080: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.7834 - ACC: 0.1721 - PPL: 328.0810 - val_loss: 5.6402 - val_ACC: 0.1837 - val_PPL: 285.4235\n",
      "Epoch 81/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7415 - ACC: 0.1774 - PPL: 314.2032\n",
      "Epoch 00081: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.7400 - ACC: 0.1779 - PPL: 313.6993 - val_loss: 5.6478 - val_ACC: 0.1857 - val_PPL: 288.5264\n",
      "Epoch 82/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7385 - ACC: 0.1811 - PPL: 314.0163\n",
      "Epoch 00082: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 5.7412 - ACC: 0.1810 - PPL: 314.8569 - val_loss: 5.6312 - val_ACC: 0.1855 - val_PPL: 283.3340\n",
      "Epoch 83/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7624 - ACC: 0.1760 - PPL: 321.3391\n",
      "Epoch 00083: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.7617 - ACC: 0.1759 - PPL: 321.0622 - val_loss: 5.6703 - val_ACC: 0.1760 - val_PPL: 292.1375\n",
      "Epoch 84/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7535 - ACC: 0.1771 - PPL: 318.3038\n",
      "Epoch 00084: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.7517 - ACC: 0.1772 - PPL: 317.6979 - val_loss: 5.6620 - val_ACC: 0.1768 - val_PPL: 291.1936\n",
      "Epoch 85/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7894 - ACC: 0.1755 - PPL: 329.6567\n",
      "Epoch 00085: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 328ms/step - loss: 5.7960 - ACC: 0.1749 - PPL: 332.1204 - val_loss: 5.5933 - val_ACC: 0.1920 - val_PPL: 273.2541\n",
      "Epoch 86/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7430 - ACC: 0.1795 - PPL: 315.7877\n",
      "Epoch 00086: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 333ms/step - loss: 5.7469 - ACC: 0.1792 - PPL: 317.0766 - val_loss: 5.6058 - val_ACC: 0.1881 - val_PPL: 276.3057\n",
      "Epoch 87/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7381 - ACC: 0.1791 - PPL: 314.7347\n",
      "Epoch 00087: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 331ms/step - loss: 5.7369 - ACC: 0.1791 - PPL: 314.2707 - val_loss: 5.6850 - val_ACC: 0.1825 - val_PPL: 298.3224\n",
      "Epoch 88/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7773 - ACC: 0.1777 - PPL: 328.3126\n",
      "Epoch 00088: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.7784 - ACC: 0.1774 - PPL: 328.5612 - val_loss: 5.5753 - val_ACC: 0.1881 - val_PPL: 267.8443\n",
      "Epoch 89/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6713 - ACC: 0.1887 - PPL: 295.4651\n",
      "Epoch 00089: val_PPL improved from 267.34196 to 265.85492, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.6726 - ACC: 0.1883 - PPL: 295.7431 - val_loss: 5.5738 - val_ACC: 0.1914 - val_PPL: 265.8549\n",
      "Epoch 90/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6989 - ACC: 0.1862 - PPL: 302.0493\n",
      "Epoch 00090: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 5.6957 - ACC: 0.1864 - PPL: 301.0924 - val_loss: 5.5785 - val_ACC: 0.1910 - val_PPL: 266.9456\n",
      "Epoch 91/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7501 - ACC: 0.1784 - PPL: 318.0720\n",
      "Epoch 00091: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 332ms/step - loss: 5.7489 - ACC: 0.1788 - PPL: 317.6391 - val_loss: 5.6589 - val_ACC: 0.1810 - val_PPL: 290.0278\n",
      "Epoch 92/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7250 - ACC: 0.1840 - PPL: 309.3717\n",
      "Epoch 00092: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.7208 - ACC: 0.1847 - PPL: 308.1445 - val_loss: 5.6056 - val_ACC: 0.1907 - val_PPL: 275.2138\n",
      "Epoch 93/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7419 - ACC: 0.1776 - PPL: 315.3518\n",
      "Epoch 00093: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 327ms/step - loss: 5.7423 - ACC: 0.1775 - PPL: 315.4023 - val_loss: 5.6060 - val_ACC: 0.1877 - val_PPL: 276.4542\n",
      "Epoch 94/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6599 - ACC: 0.1824 - PPL: 292.2428\n",
      "Epoch 00094: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.6642 - ACC: 0.1818 - PPL: 293.5228 - val_loss: 5.6261 - val_ACC: 0.1856 - val_PPL: 281.6790\n",
      "Epoch 95/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7190 - ACC: 0.1824 - PPL: 309.6197\n",
      "Epoch 00095: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 331ms/step - loss: 5.7142 - ACC: 0.1829 - PPL: 308.2108 - val_loss: 5.5873 - val_ACC: 0.1911 - val_PPL: 270.1202\n",
      "Epoch 96/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7323 - ACC: 0.1793 - PPL: 313.1760\n",
      "Epoch 00096: val_PPL improved from 265.85492 to 261.46371, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 333ms/step - loss: 5.7260 - ACC: 0.1799 - PPL: 311.4363 - val_loss: 5.5564 - val_ACC: 0.1858 - val_PPL: 261.4637\n",
      "Epoch 97/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7452 - ACC: 0.1784 - PPL: 315.2648\n",
      "Epoch 00097: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 331ms/step - loss: 5.7464 - ACC: 0.1782 - PPL: 315.5979 - val_loss: 5.5940 - val_ACC: 0.1920 - val_PPL: 273.1780\n",
      "Epoch 98/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7041 - ACC: 0.1859 - PPL: 304.4843\n",
      "Epoch 00098: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 328ms/step - loss: 5.7051 - ACC: 0.1859 - PPL: 304.6968 - val_loss: 5.6255 - val_ACC: 0.1847 - val_PPL: 279.6183\n",
      "Epoch 99/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7006 - ACC: 0.1828 - PPL: 303.0761\n",
      "Epoch 00099: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 330ms/step - loss: 5.6959 - ACC: 0.1831 - PPL: 301.7373 - val_loss: 5.6211 - val_ACC: 0.1895 - val_PPL: 278.2935\n",
      "Epoch 100/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7046 - ACC: 0.1865 - PPL: 305.3090\n",
      "Epoch 00100: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 335ms/step - loss: 5.7048 - ACC: 0.1867 - PPL: 305.2951 - val_loss: 5.5892 - val_ACC: 0.1937 - val_PPL: 272.5413\n",
      "Epoch 101/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6999 - ACC: 0.1850 - PPL: 304.6087\n",
      "Epoch 00101: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.7034 - ACC: 0.1845 - PPL: 305.6390 - val_loss: 5.7121 - val_ACC: 0.1791 - val_PPL: 305.5145\n",
      "Epoch 102/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7185 - ACC: 0.1846 - PPL: 308.3196\n",
      "Epoch 00102: val_PPL improved from 261.46371 to 259.12334, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 328ms/step - loss: 5.7235 - ACC: 0.1841 - PPL: 309.9707 - val_loss: 5.5423 - val_ACC: 0.1942 - val_PPL: 259.1233\n",
      "Epoch 103/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7568 - ACC: 0.1771 - PPL: 320.4198\n",
      "Epoch 00103: val_PPL improved from 259.12334 to 253.93278, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 327ms/step - loss: 5.7559 - ACC: 0.1770 - PPL: 320.0514 - val_loss: 5.5197 - val_ACC: 0.1968 - val_PPL: 253.9328\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/50 [============================>.] - ETA: 0s - loss: 5.6905 - ACC: 0.1844 - PPL: 300.2569\n",
      "Epoch 00104: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.6852 - ACC: 0.1848 - PPL: 298.7936 - val_loss: 5.6097 - val_ACC: 0.1871 - val_PPL: 278.0303\n",
      "Epoch 105/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7306 - ACC: 0.1820 - PPL: 312.3549\n",
      "Epoch 00105: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 335ms/step - loss: 5.7331 - ACC: 0.1819 - PPL: 313.0928 - val_loss: 5.5754 - val_ACC: 0.1931 - val_PPL: 267.4841\n",
      "Epoch 106/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7276 - ACC: 0.1817 - PPL: 310.6750\n",
      "Epoch 00106: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.7293 - ACC: 0.1820 - PPL: 311.1567 - val_loss: 5.5354 - val_ACC: 0.1961 - val_PPL: 256.5004\n",
      "Epoch 107/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7009 - ACC: 0.1859 - PPL: 302.2813\n",
      "Epoch 00107: val_PPL improved from 253.93278 to 249.64519, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 330ms/step - loss: 5.7050 - ACC: 0.1857 - PPL: 303.5873 - val_loss: 5.5037 - val_ACC: 0.1987 - val_PPL: 249.6452\n",
      "Epoch 108/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6922 - ACC: 0.1872 - PPL: 299.9841\n",
      "Epoch 00108: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.6960 - ACC: 0.1868 - PPL: 301.1294 - val_loss: 5.5782 - val_ACC: 0.1974 - val_PPL: 268.0500\n",
      "Epoch 109/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6658 - ACC: 0.1860 - PPL: 292.4184\n",
      "Epoch 00109: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.6683 - ACC: 0.1857 - PPL: 293.1154 - val_loss: 5.6027 - val_ACC: 0.1901 - val_PPL: 274.3605\n",
      "Epoch 110/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7255 - ACC: 0.1790 - PPL: 309.5248\n",
      "Epoch 00110: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.7228 - ACC: 0.1794 - PPL: 308.7055 - val_loss: 5.5695 - val_ACC: 0.1986 - val_PPL: 265.3249\n",
      "Epoch 111/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6651 - ACC: 0.1901 - PPL: 291.6765\n",
      "Epoch 00111: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 328ms/step - loss: 5.6606 - ACC: 0.1904 - PPL: 290.4514 - val_loss: 5.5403 - val_ACC: 0.1932 - val_PPL: 258.1491\n",
      "Epoch 112/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7199 - ACC: 0.1813 - PPL: 308.9940\n",
      "Epoch 00112: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.7153 - ACC: 0.1819 - PPL: 307.6510 - val_loss: 5.5595 - val_ACC: 0.1929 - val_PPL: 263.5913\n",
      "Epoch 113/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6447 - ACC: 0.1901 - PPL: 286.9321\n",
      "Epoch 00113: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.6393 - ACC: 0.1905 - PPL: 285.5124 - val_loss: 5.5310 - val_ACC: 0.1961 - val_PPL: 255.9896\n",
      "Epoch 114/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6288 - ACC: 0.1925 - PPL: 282.8166\n",
      "Epoch 00114: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6350 - ACC: 0.1920 - PPL: 284.7449 - val_loss: 5.5974 - val_ACC: 0.1898 - val_PPL: 273.0042\n",
      "Epoch 115/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6864 - ACC: 0.1875 - PPL: 297.8510\n",
      "Epoch 00115: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.6863 - ACC: 0.1876 - PPL: 297.7876 - val_loss: 5.5409 - val_ACC: 0.1948 - val_PPL: 258.0528\n",
      "Epoch 116/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6789 - ACC: 0.1845 - PPL: 296.3836\n",
      "Epoch 00116: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.6785 - ACC: 0.1844 - PPL: 296.2060 - val_loss: 5.5969 - val_ACC: 0.1914 - val_PPL: 271.4421\n",
      "Epoch 117/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6850 - ACC: 0.1853 - PPL: 298.9073\n",
      "Epoch 00117: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 5.6879 - ACC: 0.1849 - PPL: 299.7456 - val_loss: 5.5473 - val_ACC: 0.1968 - val_PPL: 259.1734\n",
      "Epoch 118/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6883 - ACC: 0.1886 - PPL: 299.6060\n",
      "Epoch 00118: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.6922 - ACC: 0.1873 - PPL: 300.7624 - val_loss: 5.5882 - val_ACC: 0.1889 - val_PPL: 269.9547\n",
      "Epoch 119/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6933 - ACC: 0.1859 - PPL: 300.1036\n",
      "Epoch 00119: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6886 - ACC: 0.1864 - PPL: 298.7939 - val_loss: 5.5411 - val_ACC: 0.1954 - val_PPL: 259.5479\n",
      "Epoch 120/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6417 - ACC: 0.1895 - PPL: 285.7360\n",
      "Epoch 00120: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6411 - ACC: 0.1894 - PPL: 285.5086 - val_loss: 5.5622 - val_ACC: 0.1932 - val_PPL: 264.0236\n",
      "Epoch 121/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.7020 - ACC: 0.1842 - PPL: 302.3960\n",
      "Epoch 00121: val_PPL did not improve\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 5.6973 - ACC: 0.1849 - PPL: 301.0798 - val_loss: 5.5563 - val_ACC: 0.1937 - val_PPL: 261.1223\n",
      "Epoch 122/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6632 - ACC: 0.1876 - PPL: 292.4802\n",
      "Epoch 00122: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.6571 - ACC: 0.1883 - PPL: 290.8821 - val_loss: 5.5616 - val_ACC: 0.1931 - val_PPL: 263.0046\n",
      "Epoch 123/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6619 - ACC: 0.1850 - PPL: 290.6734\n",
      "Epoch 00123: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6578 - ACC: 0.1857 - PPL: 289.5487 - val_loss: 5.5385 - val_ACC: 0.1977 - val_PPL: 259.1211\n",
      "Epoch 124/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6818 - ACC: 0.1845 - PPL: 298.1883\n",
      "Epoch 00124: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.6838 - ACC: 0.1841 - PPL: 298.7002 - val_loss: 5.5891 - val_ACC: 0.1948 - val_PPL: 271.9148\n",
      "Epoch 125/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6821 - ACC: 0.1868 - PPL: 296.2994\n",
      "Epoch 00125: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6819 - ACC: 0.1866 - PPL: 296.1888 - val_loss: 5.5776 - val_ACC: 0.1943 - val_PPL: 266.9598\n",
      "Epoch 126/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6478 - ACC: 0.1885 - PPL: 287.5785\n",
      "Epoch 00126: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.6484 - ACC: 0.1876 - PPL: 287.6951 - val_loss: 5.6089 - val_ACC: 0.1904 - val_PPL: 276.2294\n",
      "Epoch 127/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6682 - ACC: 0.1869 - PPL: 292.7520\n",
      "Epoch 00127: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6712 - ACC: 0.1865 - PPL: 293.6131 - val_loss: 5.5979 - val_ACC: 0.1924 - val_PPL: 273.0238\n",
      "Epoch 128/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6687 - ACC: 0.1849 - PPL: 293.0708\n",
      "Epoch 00128: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.6686 - ACC: 0.1848 - PPL: 292.9876 - val_loss: 5.5565 - val_ACC: 0.1948 - val_PPL: 260.8332\n",
      "Epoch 129/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6763 - ACC: 0.1861 - PPL: 295.7139\n",
      "Epoch 00129: val_PPL improved from 249.64519 to 243.11921, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6776 - ACC: 0.1860 - PPL: 296.0107 - val_loss: 5.4815 - val_ACC: 0.2071 - val_PPL: 243.1192\n",
      "Epoch 130/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6668 - ACC: 0.1871 - PPL: 292.8080\n",
      "Epoch 00130: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.6549 - ACC: 0.1879 - PPL: 290.1511 - val_loss: 5.5031 - val_ACC: 0.1993 - val_PPL: 247.9232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6582 - ACC: 0.1876 - PPL: 289.4965\n",
      "Epoch 00131: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 327ms/step - loss: 5.6591 - ACC: 0.1871 - PPL: 289.7180 - val_loss: 5.5380 - val_ACC: 0.1929 - val_PPL: 258.0050\n",
      "Epoch 132/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6396 - ACC: 0.1935 - PPL: 286.6907\n",
      "Epoch 00132: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.6452 - ACC: 0.1925 - PPL: 288.3932 - val_loss: 5.5399 - val_ACC: 0.1945 - val_PPL: 258.2285\n",
      "Epoch 133/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6199 - ACC: 0.1915 - PPL: 279.2806\n",
      "Epoch 00133: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6210 - ACC: 0.1907 - PPL: 279.5286 - val_loss: 5.5261 - val_ACC: 0.1963 - val_PPL: 255.8539\n",
      "Epoch 134/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6140 - ACC: 0.1886 - PPL: 277.7141\n",
      "Epoch 00134: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6160 - ACC: 0.1883 - PPL: 278.1978 - val_loss: 5.5191 - val_ACC: 0.1956 - val_PPL: 252.2763\n",
      "Epoch 135/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6639 - ACC: 0.1900 - PPL: 292.0237\n",
      "Epoch 00135: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6638 - ACC: 0.1896 - PPL: 291.9398 - val_loss: 5.5012 - val_ACC: 0.2045 - val_PPL: 249.1371\n",
      "Epoch 136/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6329 - ACC: 0.1904 - PPL: 283.1857\n",
      "Epoch 00136: val_PPL improved from 243.11921 to 236.15993, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6288 - ACC: 0.1906 - PPL: 282.0672 - val_loss: 5.4470 - val_ACC: 0.2092 - val_PPL: 236.1599\n",
      "Epoch 137/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6301 - ACC: 0.1912 - PPL: 282.2683\n",
      "Epoch 00137: val_PPL did not improve\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 5.6298 - ACC: 0.1915 - PPL: 282.1174 - val_loss: 5.5171 - val_ACC: 0.2034 - val_PPL: 254.3667\n",
      "Epoch 138/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6592 - ACC: 0.1880 - PPL: 290.1676\n",
      "Epoch 00138: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6626 - ACC: 0.1874 - PPL: 291.1841 - val_loss: 5.5007 - val_ACC: 0.1986 - val_PPL: 248.8113\n",
      "Epoch 139/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6148 - ACC: 0.1922 - PPL: 277.1845\n",
      "Epoch 00139: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6156 - ACC: 0.1925 - PPL: 277.3559 - val_loss: 5.6079 - val_ACC: 0.1926 - val_PPL: 276.8290\n",
      "Epoch 140/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6195 - ACC: 0.1905 - PPL: 280.3356\n",
      "Epoch 00140: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6196 - ACC: 0.1902 - PPL: 280.2573 - val_loss: 5.5232 - val_ACC: 0.1987 - val_PPL: 251.9719\n",
      "Epoch 141/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6374 - ACC: 0.1909 - PPL: 285.4097\n",
      "Epoch 00141: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.6371 - ACC: 0.1911 - PPL: 285.2294 - val_loss: 5.4832 - val_ACC: 0.2026 - val_PPL: 245.0462\n",
      "Epoch 142/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6250 - ACC: 0.1913 - PPL: 281.6563\n",
      "Epoch 00142: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6199 - ACC: 0.1914 - PPL: 280.3125 - val_loss: 5.5478 - val_ACC: 0.1990 - val_PPL: 260.1641\n",
      "Epoch 143/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6283 - ACC: 0.1880 - PPL: 282.1027\n",
      "Epoch 00143: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 5.6315 - ACC: 0.1876 - PPL: 282.9918 - val_loss: 5.5480 - val_ACC: 0.1991 - val_PPL: 259.8617\n",
      "Epoch 144/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6245 - ACC: 0.1885 - PPL: 281.7117\n",
      "Epoch 00144: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6225 - ACC: 0.1888 - PPL: 281.1014 - val_loss: 5.4565 - val_ACC: 0.2092 - val_PPL: 239.1013\n",
      "Epoch 145/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6658 - ACC: 0.1893 - PPL: 292.9909\n",
      "Epoch 00145: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6597 - ACC: 0.1898 - PPL: 291.3850 - val_loss: 5.5026 - val_ACC: 0.2010 - val_PPL: 249.2798\n",
      "Epoch 146/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6395 - ACC: 0.1916 - PPL: 286.0253\n",
      "Epoch 00146: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.6395 - ACC: 0.1915 - PPL: 285.9220 - val_loss: 5.5126 - val_ACC: 0.1990 - val_PPL: 250.9740\n",
      "Epoch 147/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6607 - ACC: 0.1880 - PPL: 290.6988\n",
      "Epoch 00147: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 5.6592 - ACC: 0.1880 - PPL: 290.2244 - val_loss: 5.5232 - val_ACC: 0.2017 - val_PPL: 252.7979\n",
      "Epoch 148/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6181 - ACC: 0.1930 - PPL: 279.5951\n",
      "Epoch 00148: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6245 - ACC: 0.1924 - PPL: 281.5685 - val_loss: 5.5408 - val_ACC: 0.1969 - val_PPL: 257.9944\n",
      "Epoch 149/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6307 - ACC: 0.1910 - PPL: 282.6695\n",
      "Epoch 00149: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.6245 - ACC: 0.1916 - PPL: 281.1092 - val_loss: 5.5327 - val_ACC: 0.1974 - val_PPL: 257.9626\n",
      "Epoch 150/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6159 - ACC: 0.1912 - PPL: 278.4626\n",
      "Epoch 00150: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.6170 - ACC: 0.1915 - PPL: 278.7153 - val_loss: 5.5262 - val_ACC: 0.1965 - val_PPL: 255.1978\n",
      "Epoch 151/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5699 - ACC: 0.1972 - PPL: 266.7697\n",
      "Epoch 00151: val_PPL improved from 236.15993 to 232.56144, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 17s 336ms/step - loss: 5.5740 - ACC: 0.1967 - PPL: 267.8922 - val_loss: 5.4363 - val_ACC: 0.2104 - val_PPL: 232.5614\n",
      "Epoch 152/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6167 - ACC: 0.1940 - PPL: 279.0062\n",
      "Epoch 00152: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6182 - ACC: 0.1937 - PPL: 279.3559 - val_loss: 5.6017 - val_ACC: 0.1894 - val_PPL: 274.2667\n",
      "Epoch 153/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6246 - ACC: 0.1914 - PPL: 280.5127\n",
      "Epoch 00153: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 5.6220 - ACC: 0.1911 - PPL: 279.7658 - val_loss: 5.5952 - val_ACC: 0.1896 - val_PPL: 273.8889\n",
      "Epoch 154/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6183 - ACC: 0.1934 - PPL: 278.8764\n",
      "Epoch 00154: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6210 - ACC: 0.1929 - PPL: 279.5826 - val_loss: 5.5391 - val_ACC: 0.2020 - val_PPL: 259.0123\n",
      "Epoch 155/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6211 - ACC: 0.1942 - PPL: 279.4895\n",
      "Epoch 00155: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6215 - ACC: 0.1943 - PPL: 279.5304 - val_loss: 5.4794 - val_ACC: 0.2060 - val_PPL: 243.5814\n",
      "Epoch 156/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6054 - ACC: 0.1957 - PPL: 276.7216\n",
      "Epoch 00156: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6003 - ACC: 0.1960 - PPL: 275.4128 - val_loss: 5.5163 - val_ACC: 0.1978 - val_PPL: 253.0825\n",
      "Epoch 157/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6184 - ACC: 0.1927 - PPL: 280.0351\n",
      "Epoch 00157: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6184 - ACC: 0.1929 - PPL: 279.9495 - val_loss: 5.5177 - val_ACC: 0.2013 - val_PPL: 254.9858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6368 - ACC: 0.1897 - PPL: 284.1868\n",
      "Epoch 00158: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6381 - ACC: 0.1897 - PPL: 284.4633 - val_loss: 5.5189 - val_ACC: 0.1998 - val_PPL: 252.8694\n",
      "Epoch 159/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6383 - ACC: 0.1906 - PPL: 285.0553\n",
      "Epoch 00159: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6390 - ACC: 0.1903 - PPL: 285.1766 - val_loss: 5.4972 - val_ACC: 0.2033 - val_PPL: 249.2483\n",
      "Epoch 160/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6320 - ACC: 0.1930 - PPL: 282.2702\n",
      "Epoch 00160: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6325 - ACC: 0.1926 - PPL: 282.3504 - val_loss: 5.4572 - val_ACC: 0.2057 - val_PPL: 237.2438\n",
      "Epoch 161/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6171 - ACC: 0.1948 - PPL: 278.8787\n",
      "Epoch 00161: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6176 - ACC: 0.1944 - PPL: 278.9391 - val_loss: 5.5099 - val_ACC: 0.2014 - val_PPL: 249.1467\n",
      "Epoch 162/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6648 - ACC: 0.1876 - PPL: 291.4487\n",
      "Epoch 00162: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6632 - ACC: 0.1874 - PPL: 290.9575 - val_loss: 5.4994 - val_ACC: 0.2023 - val_PPL: 247.7456\n",
      "Epoch 163/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6255 - ACC: 0.1902 - PPL: 281.2648\n",
      "Epoch 00163: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6282 - ACC: 0.1901 - PPL: 282.0010 - val_loss: 5.5398 - val_ACC: 0.1992 - val_PPL: 259.5669\n",
      "Epoch 164/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6265 - ACC: 0.1926 - PPL: 281.6603\n",
      "Epoch 00164: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6208 - ACC: 0.1929 - PPL: 280.2112 - val_loss: 5.5146 - val_ACC: 0.1997 - val_PPL: 252.9750\n",
      "Epoch 165/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6111 - ACC: 0.1930 - PPL: 276.3994\n",
      "Epoch 00165: val_PPL did not improve\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 5.6130 - ACC: 0.1927 - PPL: 276.8828 - val_loss: 5.4620 - val_ACC: 0.1979 - val_PPL: 240.0543\n",
      "Epoch 166/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5958 - ACC: 0.1945 - PPL: 273.1328\n",
      "Epoch 00166: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5990 - ACC: 0.1939 - PPL: 273.9889 - val_loss: 5.5727 - val_ACC: 0.1943 - val_PPL: 265.9769\n",
      "Epoch 167/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5892 - ACC: 0.1949 - PPL: 270.1701\n",
      "Epoch 00167: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.5911 - ACC: 0.1950 - PPL: 270.6582 - val_loss: 5.5191 - val_ACC: 0.1987 - val_PPL: 253.5324\n",
      "Epoch 168/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6285 - ACC: 0.1934 - PPL: 280.7539\n",
      "Epoch 00168: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.6301 - ACC: 0.1934 - PPL: 281.1673 - val_loss: 5.5122 - val_ACC: 0.2036 - val_PPL: 251.0244\n",
      "Epoch 169/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5939 - ACC: 0.1904 - PPL: 272.6761\n",
      "Epoch 00169: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5957 - ACC: 0.1904 - PPL: 273.1240 - val_loss: 5.6018 - val_ACC: 0.1937 - val_PPL: 275.4844\n",
      "Epoch 170/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6101 - ACC: 0.1948 - PPL: 277.3193\n",
      "Epoch 00170: val_PPL improved from 232.56144 to 230.24434, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6145 - ACC: 0.1940 - PPL: 278.5822 - val_loss: 5.4329 - val_ACC: 0.2062 - val_PPL: 230.2443\n",
      "Epoch 171/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6059 - ACC: 0.1945 - PPL: 275.9391\n",
      "Epoch 00171: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.6072 - ACC: 0.1941 - PPL: 276.2244 - val_loss: 5.4725 - val_ACC: 0.2088 - val_PPL: 240.1761\n",
      "Epoch 172/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5856 - ACC: 0.1931 - PPL: 270.0616\n",
      "Epoch 00172: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5855 - ACC: 0.1932 - PPL: 269.9658 - val_loss: 5.4897 - val_ACC: 0.2033 - val_PPL: 245.7164\n",
      "Epoch 173/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6088 - ACC: 0.1933 - PPL: 275.5134\n",
      "Epoch 00173: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.6118 - ACC: 0.1929 - PPL: 276.3453 - val_loss: 5.4992 - val_ACC: 0.2038 - val_PPL: 247.4524\n",
      "Epoch 174/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6470 - ACC: 0.1907 - PPL: 288.4072\n",
      "Epoch 00174: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.6424 - ACC: 0.1908 - PPL: 287.1596 - val_loss: 5.4575 - val_ACC: 0.2100 - val_PPL: 237.4477\n",
      "Epoch 175/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5494 - ACC: 0.1995 - PPL: 259.9442\n",
      "Epoch 00175: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5566 - ACC: 0.1991 - PPL: 262.0871 - val_loss: 5.4307 - val_ACC: 0.2120 - val_PPL: 231.3763\n",
      "Epoch 176/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5601 - ACC: 0.1976 - PPL: 263.6170\n",
      "Epoch 00176: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.5643 - ACC: 0.1975 - PPL: 264.7331 - val_loss: 5.5223 - val_ACC: 0.1988 - val_PPL: 253.0742\n",
      "Epoch 177/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5938 - ACC: 0.1925 - PPL: 271.7431\n",
      "Epoch 00177: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5998 - ACC: 0.1920 - PPL: 273.5395 - val_loss: 5.5368 - val_ACC: 0.1980 - val_PPL: 258.0733\n",
      "Epoch 178/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6014 - ACC: 0.1912 - PPL: 273.9799\n",
      "Epoch 00178: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.6022 - ACC: 0.1910 - PPL: 274.1417 - val_loss: 5.5230 - val_ACC: 0.1976 - val_PPL: 253.7987\n",
      "Epoch 179/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5904 - ACC: 0.1944 - PPL: 272.1100\n",
      "Epoch 00179: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.5949 - ACC: 0.1942 - PPL: 273.3698 - val_loss: 5.5358 - val_ACC: 0.1973 - val_PPL: 257.9358\n",
      "Epoch 180/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6336 - ACC: 0.1935 - PPL: 283.3714\n",
      "Epoch 00180: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.6370 - ACC: 0.1929 - PPL: 284.3524 - val_loss: 5.5173 - val_ACC: 0.2027 - val_PPL: 252.1332\n",
      "Epoch 181/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6187 - ACC: 0.1917 - PPL: 278.1227\n",
      "Epoch 00181: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6222 - ACC: 0.1913 - PPL: 279.1378 - val_loss: 5.4606 - val_ACC: 0.2042 - val_PPL: 237.7177\n",
      "Epoch 182/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6372 - ACC: 0.1908 - PPL: 283.6242\n",
      "Epoch 00182: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6324 - ACC: 0.1914 - PPL: 282.3750 - val_loss: 5.5468 - val_ACC: 0.2004 - val_PPL: 260.2977\n",
      "Epoch 183/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5940 - ACC: 0.1987 - PPL: 272.0068\n",
      "Epoch 00183: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.5973 - ACC: 0.1983 - PPL: 272.9024 - val_loss: 5.4787 - val_ACC: 0.2069 - val_PPL: 243.3077\n",
      "Epoch 184/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5415 - ACC: 0.1986 - PPL: 258.1138\n",
      "Epoch 00184: val_PPL improved from 230.24434 to 222.96678, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.5444 - ACC: 0.1985 - PPL: 258.8424 - val_loss: 5.3942 - val_ACC: 0.2158 - val_PPL: 222.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5852 - ACC: 0.1962 - PPL: 270.0193\n",
      "Epoch 00185: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.5839 - ACC: 0.1965 - PPL: 269.6143 - val_loss: 5.4977 - val_ACC: 0.2049 - val_PPL: 250.5028\n",
      "Epoch 186/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6204 - ACC: 0.1941 - PPL: 280.4779\n",
      "Epoch 00186: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6228 - ACC: 0.1940 - PPL: 281.0937 - val_loss: 5.5199 - val_ACC: 0.2019 - val_PPL: 253.8240\n",
      "Epoch 187/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5490 - ACC: 0.1982 - PPL: 260.2732\n",
      "Epoch 00187: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5440 - ACC: 0.1988 - PPL: 259.0642 - val_loss: 5.4823 - val_ACC: 0.2094 - val_PPL: 244.0693\n",
      "Epoch 188/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5732 - ACC: 0.1996 - PPL: 268.3572\n",
      "Epoch 00188: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.5741 - ACC: 0.1994 - PPL: 268.5069 - val_loss: 5.4984 - val_ACC: 0.2070 - val_PPL: 246.5038\n",
      "Epoch 189/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5824 - ACC: 0.1935 - PPL: 269.3970\n",
      "Epoch 00189: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5881 - ACC: 0.1929 - PPL: 271.0706 - val_loss: 5.5043 - val_ACC: 0.2041 - val_PPL: 250.3450\n",
      "Epoch 190/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5964 - ACC: 0.1960 - PPL: 274.0071\n",
      "Epoch 00190: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5979 - ACC: 0.1954 - PPL: 274.3410 - val_loss: 5.4708 - val_ACC: 0.2051 - val_PPL: 240.6155\n",
      "Epoch 191/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5735 - ACC: 0.1959 - PPL: 266.7085\n",
      "Epoch 00191: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5724 - ACC: 0.1963 - PPL: 266.3670 - val_loss: 5.4665 - val_ACC: 0.2105 - val_PPL: 240.9204\n",
      "Epoch 192/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5509 - ACC: 0.1997 - PPL: 260.9495\n",
      "Epoch 00192: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5475 - ACC: 0.2001 - PPL: 260.0802 - val_loss: 5.5214 - val_ACC: 0.1987 - val_PPL: 254.6035\n",
      "Epoch 193/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5595 - ACC: 0.1981 - PPL: 264.7108\n",
      "Epoch 00193: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.5576 - ACC: 0.1976 - PPL: 264.1382 - val_loss: 5.4696 - val_ACC: 0.2093 - val_PPL: 242.2663\n",
      "Epoch 194/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5717 - ACC: 0.1942 - PPL: 266.0537\n",
      "Epoch 00194: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.5669 - ACC: 0.1944 - PPL: 264.8594 - val_loss: 5.4517 - val_ACC: 0.2055 - val_PPL: 236.9782\n",
      "Epoch 195/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5531 - ACC: 0.1989 - PPL: 261.1632\n",
      "Epoch 00195: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 5.5527 - ACC: 0.1993 - PPL: 260.9903 - val_loss: 5.4098 - val_ACC: 0.2155 - val_PPL: 227.2811\n",
      "Epoch 196/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5815 - ACC: 0.1985 - PPL: 270.3892\n",
      "Epoch 00196: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.5809 - ACC: 0.1984 - PPL: 270.1535 - val_loss: 5.5544 - val_ACC: 0.1977 - val_PPL: 262.2771\n",
      "Epoch 197/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5966 - ACC: 0.1929 - PPL: 273.6883\n",
      "Epoch 00197: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5941 - ACC: 0.1929 - PPL: 272.9814 - val_loss: 5.5072 - val_ACC: 0.2012 - val_PPL: 250.6964\n",
      "Epoch 198/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5826 - ACC: 0.2000 - PPL: 270.2534\n",
      "Epoch 00198: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.5881 - ACC: 0.1996 - PPL: 271.8297 - val_loss: 5.5853 - val_ACC: 0.1887 - val_PPL: 270.3684\n",
      "Epoch 199/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5665 - ACC: 0.1985 - PPL: 264.3036\n",
      "Epoch 00199: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.5657 - ACC: 0.1984 - PPL: 264.0444 - val_loss: 5.4514 - val_ACC: 0.2151 - val_PPL: 236.7030\n",
      "Epoch 200/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5864 - ACC: 0.1974 - PPL: 271.2762\n",
      "Epoch 00200: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 330ms/step - loss: 5.5879 - ACC: 0.1973 - PPL: 271.6279 - val_loss: 5.3977 - val_ACC: 0.2160 - val_PPL: 225.1814\n",
      "Epoch 201/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5932 - ACC: 0.1948 - PPL: 271.5466\n",
      "Epoch 00201: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 5.5932 - ACC: 0.1948 - PPL: 271.4923 - val_loss: 5.3944 - val_ACC: 0.2130 - val_PPL: 223.3964\n",
      "Epoch 202/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6000 - ACC: 0.1974 - PPL: 274.5430\n",
      "Epoch 00202: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.6034 - ACC: 0.1971 - PPL: 275.4421 - val_loss: 5.4263 - val_ACC: 0.2087 - val_PPL: 231.6729\n",
      "Epoch 203/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5453 - ACC: 0.2008 - PPL: 260.7025\n",
      "Epoch 00203: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 5.5434 - ACC: 0.2015 - PPL: 260.1339 - val_loss: 5.5252 - val_ACC: 0.1994 - val_PPL: 253.8131\n",
      "Epoch 204/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5842 - ACC: 0.1981 - PPL: 268.7924\n",
      "Epoch 00204: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5811 - ACC: 0.1982 - PPL: 267.9790 - val_loss: 5.4641 - val_ACC: 0.2100 - val_PPL: 239.7868\n",
      "Epoch 205/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5925 - ACC: 0.1948 - PPL: 271.0399\n",
      "Epoch 00205: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.5871 - ACC: 0.1952 - PPL: 269.7034 - val_loss: 5.4400 - val_ACC: 0.2139 - val_PPL: 234.3863\n",
      "Epoch 206/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5700 - ACC: 0.1982 - PPL: 266.2006\n",
      "Epoch 00206: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5713 - ACC: 0.1978 - PPL: 266.5005 - val_loss: 5.4675 - val_ACC: 0.2057 - val_PPL: 240.0811\n",
      "Epoch 207/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5956 - ACC: 0.1972 - PPL: 273.4797\n",
      "Epoch 00207: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.5991 - ACC: 0.1966 - PPL: 274.4067 - val_loss: 5.4515 - val_ACC: 0.2026 - val_PPL: 236.0407\n",
      "Epoch 208/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5640 - ACC: 0.1973 - PPL: 264.2704\n",
      "Epoch 00208: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5659 - ACC: 0.1971 - PPL: 264.7335 - val_loss: 5.4385 - val_ACC: 0.2090 - val_PPL: 233.4549\n",
      "Epoch 209/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5849 - ACC: 0.1974 - PPL: 271.3034\n",
      "Epoch 00209: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5848 - ACC: 0.1975 - PPL: 271.1775 - val_loss: 5.4261 - val_ACC: 0.2072 - val_PPL: 232.2041\n",
      "Epoch 210/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5884 - ACC: 0.1934 - PPL: 270.6467\n",
      "Epoch 00210: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.5921 - ACC: 0.1928 - PPL: 271.6679 - val_loss: 5.5349 - val_ACC: 0.2005 - val_PPL: 256.6075\n",
      "Epoch 211/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6072 - ACC: 0.1936 - PPL: 274.3850\n",
      "Epoch 00211: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.6066 - ACC: 0.1941 - PPL: 274.1716 - val_loss: 5.5169 - val_ACC: 0.2042 - val_PPL: 253.6958\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/50 [============================>.] - ETA: 0s - loss: 5.5939 - ACC: 0.1963 - PPL: 274.4938\n",
      "Epoch 00212: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5953 - ACC: 0.1959 - PPL: 274.7690 - val_loss: 5.5690 - val_ACC: 0.1987 - val_PPL: 266.8402\n",
      "Epoch 213/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5624 - ACC: 0.1991 - PPL: 263.8923\n",
      "Epoch 00213: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5652 - ACC: 0.1988 - PPL: 264.6128 - val_loss: 5.5116 - val_ACC: 0.2041 - val_PPL: 249.9552\n",
      "Epoch 214/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5995 - ACC: 0.1947 - PPL: 274.0158\n",
      "Epoch 00214: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.6018 - ACC: 0.1946 - PPL: 274.6149 - val_loss: 5.4804 - val_ACC: 0.2075 - val_PPL: 243.4509\n",
      "Epoch 215/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5388 - ACC: 0.1973 - PPL: 257.5362\n",
      "Epoch 00215: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5364 - ACC: 0.1977 - PPL: 256.8922 - val_loss: 5.5250 - val_ACC: 0.2041 - val_PPL: 253.8059\n",
      "Epoch 216/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5772 - ACC: 0.1987 - PPL: 268.0935\n",
      "Epoch 00216: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5736 - ACC: 0.1988 - PPL: 267.1514 - val_loss: 5.4563 - val_ACC: 0.2103 - val_PPL: 237.8244\n",
      "Epoch 217/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5620 - ACC: 0.1985 - PPL: 263.2601\n",
      "Epoch 00217: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.5673 - ACC: 0.1978 - PPL: 264.7626 - val_loss: 5.4059 - val_ACC: 0.2142 - val_PPL: 224.5659\n",
      "Epoch 218/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5605 - ACC: 0.1996 - PPL: 263.5832\n",
      "Epoch 00218: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.5628 - ACC: 0.1996 - PPL: 264.1219 - val_loss: 5.4536 - val_ACC: 0.2092 - val_PPL: 236.0683\n",
      "Epoch 219/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5687 - ACC: 0.1986 - PPL: 264.6388\n",
      "Epoch 00219: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.5744 - ACC: 0.1980 - PPL: 266.2910 - val_loss: 5.4324 - val_ACC: 0.2083 - val_PPL: 232.1041\n",
      "Epoch 220/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5404 - ACC: 0.2015 - PPL: 260.0745\n",
      "Epoch 00220: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5361 - ACC: 0.2020 - PPL: 258.9709 - val_loss: 5.5420 - val_ACC: 0.1970 - val_PPL: 258.8252\n",
      "Epoch 221/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5615 - ACC: 0.1996 - PPL: 263.8520\n",
      "Epoch 00221: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 333ms/step - loss: 5.5659 - ACC: 0.1992 - PPL: 265.0337 - val_loss: 5.4451 - val_ACC: 0.2084 - val_PPL: 236.2522\n",
      "Epoch 222/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5528 - ACC: 0.2012 - PPL: 262.1027\n",
      "Epoch 00222: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.5515 - ACC: 0.2007 - PPL: 261.6851 - val_loss: 5.4043 - val_ACC: 0.2185 - val_PPL: 226.7939\n",
      "Epoch 223/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5744 - ACC: 0.1979 - PPL: 267.1793\n",
      "Epoch 00223: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.5748 - ACC: 0.1983 - PPL: 267.2132 - val_loss: 5.4791 - val_ACC: 0.2037 - val_PPL: 244.8623\n",
      "Epoch 224/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5748 - ACC: 0.1947 - PPL: 266.4651\n",
      "Epoch 00224: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 5.5712 - ACC: 0.1950 - PPL: 265.5429 - val_loss: 5.4393 - val_ACC: 0.2146 - val_PPL: 234.2338\n",
      "Epoch 225/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6049 - ACC: 0.1970 - PPL: 274.7540\n",
      "Epoch 00225: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.6019 - ACC: 0.1970 - PPL: 273.9237 - val_loss: 5.4990 - val_ACC: 0.1994 - val_PPL: 246.8366\n",
      "Epoch 226/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5682 - ACC: 0.2028 - PPL: 267.5898\n",
      "Epoch 00226: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.5660 - ACC: 0.2031 - PPL: 266.9363 - val_loss: 5.4905 - val_ACC: 0.2083 - val_PPL: 246.0204\n",
      "Epoch 227/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5629 - ACC: 0.1993 - PPL: 264.7033\n",
      "Epoch 00227: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5657 - ACC: 0.1993 - PPL: 265.3915 - val_loss: 5.4656 - val_ACC: 0.2093 - val_PPL: 239.6157\n",
      "Epoch 228/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5899 - ACC: 0.1981 - PPL: 271.7631\n",
      "Epoch 00228: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.5905 - ACC: 0.1983 - PPL: 271.8447 - val_loss: 5.4089 - val_ACC: 0.2148 - val_PPL: 228.9014\n",
      "Epoch 229/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5414 - ACC: 0.2006 - PPL: 258.3319\n",
      "Epoch 00229: val_PPL improved from 222.96678 to 219.80647, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5403 - ACC: 0.2006 - PPL: 258.0061 - val_loss: 5.3732 - val_ACC: 0.2183 - val_PPL: 219.8065\n",
      "Epoch 230/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5333 - ACC: 0.2013 - PPL: 255.4898\n",
      "Epoch 00230: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5313 - ACC: 0.2015 - PPL: 254.9655 - val_loss: 5.4349 - val_ACC: 0.2091 - val_PPL: 232.0994\n",
      "Epoch 231/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5295 - ACC: 0.2003 - PPL: 254.7167\n",
      "Epoch 00231: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5262 - ACC: 0.2007 - PPL: 253.9045 - val_loss: 5.4340 - val_ACC: 0.2087 - val_PPL: 232.5213\n",
      "Epoch 232/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5806 - ACC: 0.1981 - PPL: 269.6670\n",
      "Epoch 00232: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 327ms/step - loss: 5.5812 - ACC: 0.1982 - PPL: 269.7454 - val_loss: 5.4292 - val_ACC: 0.2106 - val_PPL: 233.9563\n",
      "Epoch 233/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5825 - ACC: 0.1980 - PPL: 268.1365\n",
      "Epoch 00233: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5849 - ACC: 0.1979 - PPL: 268.7495 - val_loss: 5.4739 - val_ACC: 0.2097 - val_PPL: 241.6485\n",
      "Epoch 234/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5778 - ACC: 0.1978 - PPL: 269.4521\n",
      "Epoch 00234: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5734 - ACC: 0.1982 - PPL: 268.3127 - val_loss: 5.4287 - val_ACC: 0.2090 - val_PPL: 230.8715\n",
      "Epoch 235/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5360 - ACC: 0.2052 - PPL: 256.4258\n",
      "Epoch 00235: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 310ms/step - loss: 5.5351 - ACC: 0.2056 - PPL: 256.1517 - val_loss: 5.4423 - val_ACC: 0.2114 - val_PPL: 234.6041\n",
      "Epoch 236/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5523 - ACC: 0.2021 - PPL: 261.7351\n",
      "Epoch 00236: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5558 - ACC: 0.2017 - PPL: 262.6485 - val_loss: 5.3961 - val_ACC: 0.2193 - val_PPL: 222.7666\n",
      "Epoch 237/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5582 - ACC: 0.2003 - PPL: 263.7036\n",
      "Epoch 00237: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.5574 - ACC: 0.2001 - PPL: 263.4236 - val_loss: 5.4668 - val_ACC: 0.2104 - val_PPL: 240.6039\n",
      "Epoch 238/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6092 - ACC: 0.1970 - PPL: 277.6416\n",
      "Epoch 00238: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.6086 - ACC: 0.1972 - PPL: 277.3845 - val_loss: 5.4163 - val_ACC: 0.2119 - val_PPL: 228.9284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6011 - ACC: 0.1986 - PPL: 274.6297\n",
      "Epoch 00239: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.6046 - ACC: 0.1985 - PPL: 275.5755 - val_loss: 5.3842 - val_ACC: 0.2175 - val_PPL: 222.2338\n",
      "Epoch 240/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5557 - ACC: 0.2015 - PPL: 262.1205\n",
      "Epoch 00240: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5537 - ACC: 0.2019 - PPL: 261.5717 - val_loss: 5.4205 - val_ACC: 0.2129 - val_PPL: 229.1488\n",
      "Epoch 241/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5280 - ACC: 0.2020 - PPL: 254.7691\n",
      "Epoch 00241: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.5245 - ACC: 0.2024 - PPL: 253.8870 - val_loss: 5.4580 - val_ACC: 0.2116 - val_PPL: 239.5579\n",
      "Epoch 242/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5852 - ACC: 0.1947 - PPL: 269.9149\n",
      "Epoch 00242: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5848 - ACC: 0.1948 - PPL: 269.7521 - val_loss: 5.4737 - val_ACC: 0.2121 - val_PPL: 241.3090\n",
      "Epoch 243/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5460 - ACC: 0.1983 - PPL: 260.0978\n",
      "Epoch 00243: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5519 - ACC: 0.1979 - PPL: 261.7583 - val_loss: 5.4234 - val_ACC: 0.2113 - val_PPL: 230.3560\n",
      "Epoch 244/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5806 - ACC: 0.1991 - PPL: 268.9862\n",
      "Epoch 00244: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.5814 - ACC: 0.1992 - PPL: 269.1217 - val_loss: 5.4164 - val_ACC: 0.2175 - val_PPL: 228.5330\n",
      "Epoch 245/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5843 - ACC: 0.2001 - PPL: 270.5814\n",
      "Epoch 00245: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5799 - ACC: 0.2003 - PPL: 269.4328 - val_loss: 5.4683 - val_ACC: 0.2077 - val_PPL: 240.8060\n",
      "Epoch 246/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5831 - ACC: 0.1965 - PPL: 269.1934\n",
      "Epoch 00246: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5826 - ACC: 0.1963 - PPL: 269.0015 - val_loss: 5.4348 - val_ACC: 0.2098 - val_PPL: 233.3312\n",
      "Epoch 247/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5510 - ACC: 0.1997 - PPL: 260.1404\n",
      "Epoch 00247: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5540 - ACC: 0.1997 - PPL: 260.9283 - val_loss: 5.4369 - val_ACC: 0.2131 - val_PPL: 234.5848\n",
      "Epoch 248/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5178 - ACC: 0.2069 - PPL: 253.3521\n",
      "Epoch 00248: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5210 - ACC: 0.2068 - PPL: 254.1348 - val_loss: 5.4987 - val_ACC: 0.2050 - val_PPL: 247.1331\n",
      "Epoch 249/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5329 - ACC: 0.2030 - PPL: 256.2390\n",
      "Epoch 00249: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.5349 - ACC: 0.2037 - PPL: 256.6941 - val_loss: 5.4488 - val_ACC: 0.2091 - val_PPL: 235.4876\n",
      "Epoch 250/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5804 - ACC: 0.1994 - PPL: 268.4826\n",
      "Epoch 00250: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.5789 - ACC: 0.1999 - PPL: 268.0280 - val_loss: 5.4647 - val_ACC: 0.2114 - val_PPL: 239.5669\n",
      "Epoch 251/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5656 - ACC: 0.2011 - PPL: 264.4096\n",
      "Epoch 00251: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5578 - ACC: 0.2021 - PPL: 262.6499 - val_loss: 5.5041 - val_ACC: 0.2080 - val_PPL: 249.9969\n",
      "Epoch 252/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5951 - ACC: 0.1995 - PPL: 272.6266\n",
      "Epoch 00252: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5969 - ACC: 0.1994 - PPL: 273.0824 - val_loss: 5.5021 - val_ACC: 0.2050 - val_PPL: 249.0106\n",
      "Epoch 253/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6072 - ACC: 0.1984 - PPL: 275.3839\n",
      "Epoch 00253: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.6064 - ACC: 0.1990 - PPL: 275.1079 - val_loss: 5.4200 - val_ACC: 0.2144 - val_PPL: 229.3391\n",
      "Epoch 254/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5928 - ACC: 0.2013 - PPL: 272.8605\n",
      "Epoch 00254: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5895 - ACC: 0.2010 - PPL: 271.9572 - val_loss: 5.4116 - val_ACC: 0.2185 - val_PPL: 226.5512\n",
      "Epoch 255/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5867 - ACC: 0.2003 - PPL: 269.1608\n",
      "Epoch 00255: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5937 - ACC: 0.1999 - PPL: 271.3569 - val_loss: 5.4603 - val_ACC: 0.2087 - val_PPL: 239.2187\n",
      "Epoch 256/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5414 - ACC: 0.2055 - PPL: 258.4944\n",
      "Epoch 00256: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.5468 - ACC: 0.2047 - PPL: 260.0044 - val_loss: 5.4452 - val_ACC: 0.2121 - val_PPL: 236.4303\n",
      "Epoch 257/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5315 - ACC: 0.2017 - PPL: 255.2108\n",
      "Epoch 00257: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.5351 - ACC: 0.2015 - PPL: 256.1512 - val_loss: 5.4361 - val_ACC: 0.2150 - val_PPL: 234.7124\n",
      "Epoch 258/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5769 - ACC: 0.1988 - PPL: 267.4480\n",
      "Epoch 00258: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5791 - ACC: 0.1982 - PPL: 268.0189 - val_loss: 5.4479 - val_ACC: 0.2105 - val_PPL: 235.2062\n",
      "Epoch 259/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5854 - ACC: 0.2001 - PPL: 270.9070\n",
      "Epoch 00259: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5876 - ACC: 0.1998 - PPL: 271.4634 - val_loss: 5.5218 - val_ACC: 0.2032 - val_PPL: 253.8007\n",
      "Epoch 260/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5592 - ACC: 0.2056 - PPL: 263.7632\n",
      "Epoch 00260: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 322ms/step - loss: 5.5600 - ACC: 0.2054 - PPL: 263.8767 - val_loss: 5.4482 - val_ACC: 0.2085 - val_PPL: 237.4286\n",
      "Epoch 261/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5426 - ACC: 0.2031 - PPL: 260.5427\n",
      "Epoch 00261: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5476 - ACC: 0.2027 - PPL: 261.8927 - val_loss: 5.4435 - val_ACC: 0.2133 - val_PPL: 235.3819\n",
      "Epoch 262/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5733 - ACC: 0.2026 - PPL: 266.9029\n",
      "Epoch 00262: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.5734 - ACC: 0.2025 - PPL: 266.8364 - val_loss: 5.4878 - val_ACC: 0.2071 - val_PPL: 245.3726\n",
      "Epoch 263/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6134 - ACC: 0.1942 - PPL: 277.8581\n",
      "Epoch 00263: val_PPL improved from 219.80647 to 213.52166, saving model to ./model/model.keras\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6145 - ACC: 0.1943 - PPL: 278.0911 - val_loss: 5.3492 - val_ACC: 0.2205 - val_PPL: 213.5217\n",
      "Epoch 264/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5525 - ACC: 0.2046 - PPL: 262.2990\n",
      "Epoch 00264: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 5.5503 - ACC: 0.2044 - PPL: 261.6786 - val_loss: 5.3860 - val_ACC: 0.2184 - val_PPL: 222.1413\n",
      "Epoch 265/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5414 - ACC: 0.2037 - PPL: 257.4353\n",
      "Epoch 00265: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 5.5432 - ACC: 0.2034 - PPL: 257.8576 - val_loss: 5.4986 - val_ACC: 0.2071 - val_PPL: 246.5691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5796 - ACC: 0.2026 - PPL: 268.7187\n",
      "Epoch 00266: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.5740 - ACC: 0.2029 - PPL: 267.3555 - val_loss: 5.4191 - val_ACC: 0.2155 - val_PPL: 228.1697\n",
      "Epoch 267/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5752 - ACC: 0.2020 - PPL: 268.1242\n",
      "Epoch 00267: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5782 - ACC: 0.2015 - PPL: 268.9170 - val_loss: 5.4792 - val_ACC: 0.2082 - val_PPL: 241.2267\n",
      "Epoch 268/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6194 - ACC: 0.1986 - PPL: 280.0842\n",
      "Epoch 00268: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.6217 - ACC: 0.1983 - PPL: 280.6663 - val_loss: 5.5351 - val_ACC: 0.1983 - val_PPL: 255.8082\n",
      "Epoch 269/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5361 - ACC: 0.2033 - PPL: 258.6592\n",
      "Epoch 00269: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5388 - ACC: 0.2031 - PPL: 259.2919 - val_loss: 5.4140 - val_ACC: 0.2138 - val_PPL: 226.5524\n",
      "Epoch 270/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5911 - ACC: 0.1995 - PPL: 273.2775\n",
      "Epoch 00270: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 312ms/step - loss: 5.5899 - ACC: 0.1999 - PPL: 272.8565 - val_loss: 5.4070 - val_ACC: 0.2189 - val_PPL: 227.1620\n",
      "Epoch 271/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5870 - ACC: 0.2022 - PPL: 269.7621\n",
      "Epoch 00271: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 317ms/step - loss: 5.5879 - ACC: 0.2016 - PPL: 269.9477 - val_loss: 5.4750 - val_ACC: 0.2096 - val_PPL: 241.9292\n",
      "Epoch 272/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5647 - ACC: 0.1977 - PPL: 265.0371\n",
      "Epoch 00272: val_PPL did not improve\n",
      "50/50 [==============================] - 15s 309ms/step - loss: 5.5607 - ACC: 0.1983 - PPL: 264.0103 - val_loss: 5.4154 - val_ACC: 0.2116 - val_PPL: 229.2375\n",
      "Epoch 273/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6059 - ACC: 0.2001 - PPL: 276.1386\n",
      "Epoch 00273: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6065 - ACC: 0.1999 - PPL: 276.2164 - val_loss: 5.5631 - val_ACC: 0.2062 - val_PPL: 264.7158\n",
      "Epoch 274/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5263 - ACC: 0.2054 - PPL: 254.2897\n",
      "Epoch 00274: val_PPL did not improve\n",
      "50/50 [==============================] - 17s 331ms/step - loss: 5.5285 - ACC: 0.2050 - PPL: 254.8246 - val_loss: 5.4187 - val_ACC: 0.2136 - val_PPL: 228.6176\n",
      "Epoch 275/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5617 - ACC: 0.2028 - PPL: 264.5005\n",
      "Epoch 00275: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5589 - ACC: 0.2028 - PPL: 263.7345 - val_loss: 5.4782 - val_ACC: 0.2103 - val_PPL: 242.1224\n",
      "Epoch 276/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5865 - ACC: 0.2032 - PPL: 270.6418\n",
      "Epoch 00276: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.5819 - ACC: 0.2033 - PPL: 269.4629 - val_loss: 5.4838 - val_ACC: 0.2122 - val_PPL: 245.7343\n",
      "Epoch 277/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5915 - ACC: 0.2003 - PPL: 272.2759\n",
      "Epoch 00277: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5906 - ACC: 0.2001 - PPL: 271.9507 - val_loss: 5.4313 - val_ACC: 0.2200 - val_PPL: 233.6973\n",
      "Epoch 278/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5095 - ACC: 0.2069 - PPL: 250.4037\n",
      "Epoch 00278: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5112 - ACC: 0.2066 - PPL: 250.7542 - val_loss: 5.4624 - val_ACC: 0.2150 - val_PPL: 239.8487\n",
      "Epoch 279/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5691 - ACC: 0.2020 - PPL: 266.6725\n",
      "Epoch 00279: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5623 - ACC: 0.2024 - PPL: 265.0609 - val_loss: 5.4270 - val_ACC: 0.2174 - val_PPL: 230.8676\n",
      "Epoch 280/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5303 - ACC: 0.2060 - PPL: 255.2854\n",
      "Epoch 00280: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5388 - ACC: 0.2056 - PPL: 257.9135 - val_loss: 5.3944 - val_ACC: 0.2204 - val_PPL: 227.1336\n",
      "Epoch 281/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5513 - ACC: 0.2061 - PPL: 261.0656\n",
      "Epoch 00281: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.5505 - ACC: 0.2061 - PPL: 260.7958 - val_loss: 5.4663 - val_ACC: 0.2094 - val_PPL: 239.2140\n",
      "Epoch 282/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5865 - ACC: 0.2028 - PPL: 271.9011\n",
      "Epoch 00282: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5884 - ACC: 0.2027 - PPL: 272.3397 - val_loss: 5.4771 - val_ACC: 0.2063 - val_PPL: 241.9050\n",
      "Epoch 283/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5378 - ACC: 0.2081 - PPL: 258.4491\n",
      "Epoch 00283: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5426 - ACC: 0.2074 - PPL: 259.7459 - val_loss: 5.5001 - val_ACC: 0.2079 - val_PPL: 249.6680\n",
      "Epoch 284/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5927 - ACC: 0.2025 - PPL: 272.7276\n",
      "Epoch 00284: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.5959 - ACC: 0.2022 - PPL: 273.5796 - val_loss: 5.4440 - val_ACC: 0.2115 - val_PPL: 235.0328\n",
      "Epoch 285/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6576 - ACC: 0.1975 - PPL: 291.1149\n",
      "Epoch 00285: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 321ms/step - loss: 5.6588 - ACC: 0.1972 - PPL: 291.3578 - val_loss: 5.4527 - val_ACC: 0.2120 - val_PPL: 236.7725\n",
      "Epoch 286/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5402 - ACC: 0.2049 - PPL: 260.4060\n",
      "Epoch 00286: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 5.5409 - ACC: 0.2049 - PPL: 260.4712 - val_loss: 5.4302 - val_ACC: 0.2159 - val_PPL: 231.5061\n",
      "Epoch 287/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6124 - ACC: 0.2005 - PPL: 277.1236\n",
      "Epoch 00287: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.6121 - ACC: 0.2004 - PPL: 276.9805 - val_loss: 5.3841 - val_ACC: 0.2200 - val_PPL: 224.2664\n",
      "Epoch 288/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5483 - ACC: 0.2044 - PPL: 260.3448\n",
      "Epoch 00288: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5511 - ACC: 0.2043 - PPL: 261.0248 - val_loss: 5.4594 - val_ACC: 0.2123 - val_PPL: 240.2457\n",
      "Epoch 289/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5530 - ACC: 0.2049 - PPL: 263.0357\n",
      "Epoch 00289: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 5.5536 - ACC: 0.2049 - PPL: 263.0962 - val_loss: 5.4844 - val_ACC: 0.2103 - val_PPL: 244.0574\n",
      "Epoch 290/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5875 - ACC: 0.2032 - PPL: 271.8124\n",
      "Epoch 00290: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5861 - ACC: 0.2034 - PPL: 271.3643 - val_loss: 5.4260 - val_ACC: 0.2186 - val_PPL: 231.4715\n",
      "Epoch 291/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6227 - ACC: 0.2002 - PPL: 280.0110\n",
      "Epoch 00291: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 324ms/step - loss: 5.6234 - ACC: 0.1997 - PPL: 280.1478 - val_loss: 5.5110 - val_ACC: 0.2056 - val_PPL: 250.5424\n",
      "Epoch 292/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5679 - ACC: 0.2043 - PPL: 266.5145\n",
      "Epoch 00292: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5617 - ACC: 0.2052 - PPL: 265.0089 - val_loss: 5.4959 - val_ACC: 0.2111 - val_PPL: 248.0640\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/50 [============================>.] - ETA: 0s - loss: 5.6211 - ACC: 0.2005 - PPL: 280.8855\n",
      "Epoch 00293: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 329ms/step - loss: 5.6164 - ACC: 0.2005 - PPL: 279.6377 - val_loss: 5.4754 - val_ACC: 0.2120 - val_PPL: 242.2648\n",
      "Epoch 294/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5694 - ACC: 0.2070 - PPL: 266.5551\n",
      "Epoch 00294: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 323ms/step - loss: 5.5605 - ACC: 0.2075 - PPL: 264.5916 - val_loss: 5.4650 - val_ACC: 0.2081 - val_PPL: 240.4875\n",
      "Epoch 295/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5838 - ACC: 0.2038 - PPL: 269.9220\n",
      "Epoch 00295: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5845 - ACC: 0.2039 - PPL: 270.0387 - val_loss: 5.4917 - val_ACC: 0.2083 - val_PPL: 246.2575\n",
      "Epoch 296/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5604 - ACC: 0.2046 - PPL: 264.4447\n",
      "Epoch 00296: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 5.5620 - ACC: 0.2047 - PPL: 264.8021 - val_loss: 5.4501 - val_ACC: 0.2183 - val_PPL: 238.2848\n",
      "Epoch 297/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5617 - ACC: 0.2096 - PPL: 265.7780\n",
      "Epoch 00297: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 318ms/step - loss: 5.5573 - ACC: 0.2098 - PPL: 264.6508 - val_loss: 5.4674 - val_ACC: 0.2115 - val_PPL: 240.0307\n",
      "Epoch 298/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.6028 - ACC: 0.2014 - PPL: 274.3088\n",
      "Epoch 00298: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 315ms/step - loss: 5.5987 - ACC: 0.2017 - PPL: 273.2420 - val_loss: 5.4783 - val_ACC: 0.2125 - val_PPL: 242.4612\n",
      "Epoch 299/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5509 - ACC: 0.2086 - PPL: 262.6773\n",
      "Epoch 00299: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 313ms/step - loss: 5.5537 - ACC: 0.2084 - PPL: 263.3341 - val_loss: 5.5053 - val_ACC: 0.2107 - val_PPL: 249.3831\n",
      "Epoch 300/300\n",
      "49/50 [============================>.] - ETA: 0s - loss: 5.5948 - ACC: 0.2047 - PPL: 272.2742\n",
      "Epoch 00300: val_PPL did not improve\n",
      "50/50 [==============================] - 16s 314ms/step - loss: 5.5944 - ACC: 0.2047 - PPL: 272.1016 - val_loss: 5.4538 - val_ACC: 0.2133 - val_PPL: 236.5528\n"
     ]
    }
   ],
   "source": [
    "hist = modelCAware.fit_generator(generator=gen_char_word(batch_size=opt.batch_size), \n",
    "                           steps_per_epoch=50, epochs=300,\n",
    "                           callbacks=callback_lists,\n",
    "                           validation_data=gen_char_word(batch_size=opt.batch_size, dataset='valid'),\n",
    "                           validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ACC', 'PPL', 'loss', 'val_ACC', 'val_PPL', 'val_loss'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f80b2603588>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = pd.DataFrame(hist.history)\n",
    "print(history.columns)\n",
    "history.loc[1:,['PPL','val_PPL']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras - Character-Aware Neural Language Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
